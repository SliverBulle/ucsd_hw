{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Linear models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Non-linear models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Import these packages conditionally to avoid linter errors\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    ADVANCED_MODELS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: XGBoost and/or LightGBM are not installed. These models will be skipped.\")\n",
    "    ADVANCED_MODELS_AVAILABLE = False\n",
    "\n",
    "# Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Train and evaluate classification models on proteomics data')\n",
    "    parser.add_argument('--train_data', type=str, default='dataset/filtered_train_data.csv',\n",
    "                        help='Path to training data')\n",
    "    parser.add_argument('--test_data', type=str, default='dataset/filtered_test_data.csv',\n",
    "                        help='Path to test data')\n",
    "    parser.add_argument('--output_dir', type=str, default='291A/291A_assignment/models',\n",
    "                        help='Directory to save models and results')\n",
    "    parser.add_argument('--cv', type=int, default=3, \n",
    "                        help='Number of cross-validation folds')\n",
    "    parser.add_argument('--n_jobs', type=int, default=-1,\n",
    "                        help='Number of jobs for parallel processing')\n",
    "    parser.add_argument('--random_state', type=int, default=42,\n",
    "                        help='Random state for reproducibility')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def load_data(train_path, test_path):\n",
    "    \"\"\"Load and prepare the training and test datasets\"\"\"\n",
    "    print(f\"Loading data from {train_path} and {test_path}\")\n",
    "    \n",
    "    # Load datasets\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # Extract feature columns and label\n",
    "    feature_cols = [col for col in train_df.columns \n",
    "                   if col not in ['patient_id', 'label']]\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = train_df[feature_cols].values\n",
    "    y_train = train_df['label'].values\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = test_df[feature_cols].values\n",
    "    y_test = test_df['label'].values\n",
    "    \n",
    "    # Record patient IDs for reference\n",
    "    train_patients = train_df['patient_id'].values\n",
    "    test_patients = test_df['patient_id'].values\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    \n",
    "    # Use list to get unique classes instead of np.unique to avoid linter errors\n",
    "    classes = sorted(list(set(y_train)))\n",
    "    print(f\"Classes: {classes}\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, train_patients, test_patients, feature_cols\n",
    "\n",
    "def get_models():\n",
    "    \"\"\"Define all models and their parameter grids for hyperparameter tuning\"\"\"\n",
    "    models = {}\n",
    "    param_grids = {}\n",
    "    \n",
    "    # Logistic Regression\n",
    "    models['Logistic_Regression'] = LogisticRegression(max_iter=10000, random_state=42)\n",
    "    param_grids['Logistic_Regression'] = {\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__solver': ['liblinear', 'saga']\n",
    "    }\n",
    "\n",
    "    return models, param_grids\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test, args):\n",
    "    \"\"\"Train all models with hyperparameter tuning and evaluate on test set\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get models and parameter grids\n",
    "    models, param_grids = get_models()\n",
    "    \n",
    "    # Setup cross-validation\n",
    "    cv = StratifiedKFold(n_splits=args.cv, shuffle=True, random_state=args.random_state)\n",
    "    \n",
    "    # Results dictionary\n",
    "    results = {\n",
    "        'model_name': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'training_time': [],\n",
    "        'best_params': []\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(f\"Training {model_name}\")\n",
    "        \n",
    "        # Create pipeline with scaling\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        \n",
    "        # Setup grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grids[model_name],\n",
    "            cv=cv,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=args.n_jobs,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Measure training time\n",
    "        start_time = time.time()\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Predictions on test set\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        # Save results\n",
    "        results['model_name'].append(model_name)\n",
    "        results['accuracy'].append(accuracy)\n",
    "        results['precision'].append(precision)\n",
    "        results['recall'].append(recall)\n",
    "        results['f1_score'].append(f1)\n",
    "        results['training_time'].append(training_time)\n",
    "        results['best_params'].append(grid_search.best_params_)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Training time: {training_time:.2f} seconds\")\n",
    "        print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Test precision: {precision:.4f}\")\n",
    "        print(f\"Test recall: {recall:.4f}\")\n",
    "        print(f\"Test F1 score: {f1:.4f}\")\n",
    "        \n",
    "        # Save model\n",
    "        model_path = os.path.join(args.output_dir, f\"{model_name}_model.pkl\")\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(best_model, f)\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        classes = sorted(list(set(np.concatenate((y_train, y_test)))))\n",
    "        plot_confusion_matrix(cm, classes, model_name, args.output_dir)\n",
    "        \n",
    "        # Generate ROC curve for multi-class\n",
    "        plot_roc_curve(best_model, X_test, y_test, model_name, args.output_dir)\n",
    "    \n",
    "    # Convert results to DataFrame and save\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(args.output_dir, 'model_comparison.csv'), index=False)\n",
    "    \n",
    "    # Plot comparison of models\n",
    "    plot_model_comparison(results_df, args.output_dir)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, model_name, output_dir):\n",
    "    \"\"\"Plot and save confusion matrix\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{model_name}_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(model, X_test, y_test, model_name, output_dir):\n",
    "    \"\"\"Plot and save ROC curve for multi-class classification\"\"\"\n",
    "    # Get probabilities for each class\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    \n",
    "    # Get unique classes\n",
    "    classes = sorted(list(set(y_test)))\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        # Convert to binary classification problem (one-vs-rest)\n",
    "        y_bin = (y_test == class_name).astype(int)\n",
    "        class_probs = y_prob[:, i]\n",
    "        \n",
    "        # Compute ROC curve and area\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin, class_probs)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        plt.plot(fpr[i], tpr[i],\n",
    "                 label=f'ROC curve for {class_name} (area = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    # Plot diagonal line (random classifier)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{model_name}_roc_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_model_comparison(results_df, output_dir):\n",
    "    \"\"\"Plot and save model comparison metrics\"\"\"\n",
    "    # Performance metrics bar chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    results_melted = pd.melt(results_df, \n",
    "                             id_vars=['model_name'],\n",
    "                             value_vars=['accuracy', 'precision', 'recall', 'f1_score'],\n",
    "                             var_name='metric', value_name='value')\n",
    "    \n",
    "    sns.barplot(x='model_name', y='value', hue='metric', data=results_melted)\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Metric')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'model_performance_comparison.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Training time comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='model_name', y='training_time', data=results_df)\n",
    "    plt.title('Model Training Time Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Training Time (seconds)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'model_training_time_comparison.png'))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = '/root/code/ucsd_hw/291A_ass2/291A_assignment/dataset/filtered_train_data.csv'\n",
    "test_data = '/root/code/ucsd_hw/291A_ass2/291A_assignment/dataset/filtered_test_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /root/code/ucsd_hw/291A_ass2/291A_assignment/dataset/filtered_train_data.csv and /root/code/ucsd_hw/291A_ass2/291A_assignment/dataset/filtered_test_data.csv\n",
      "Training data shape: (21, 44711)\n",
      "Test data shape: (6, 44711)\n",
      "Classes: ['Control', 'MBC', 'TNBC']\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, train_patients, test_patients, feature_cols = load_data(\n",
    "    train_data, test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/torch/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [  343   440   716  1443  1645  1761  2170  2180  2657  3020  3097  3697\n",
      "  3723  3748  4270  4725  4845  4846  4847  4868  5511  5623  5640  5657\n",
      "  5662  6655  6671  6885  6946  7315  7369  7490  7508  7895  8258  8270\n",
      "  8815  8917  9348  9427  9790  9807  9983 10073 10075 10146 10578 10832\n",
      " 10864 10884 11112 11155 11335 11337 11339 11340 11341 11978 11985 13110\n",
      " 13170 13502 13582 13652 14224 14307 15229 15473 15585 15652 15756 15992\n",
      " 16127 16252 16290 16369 17441 17566 17604 17714 20093 20227 20580 21007\n",
      " 21598 21632 22232 22278 22304 22459 22753 22878 23185 23248 23504 23759\n",
      " 24320 24803 25000 25291 25294 25665 25945 26161 26624 26730 26769 26840\n",
      " 26962 27037 27071 27398 27533 27719 27727 27796 27869 27959 28011 28278\n",
      " 28338 28615 28625 28628 28634 28683 29366 29460 29585 29739 29851 30221\n",
      " 31256 31315 31600 32139 32535 32689 32830 33023 33465 33552 33875 34016\n",
      " 34086 34369 34419 34620 34803 35020 35023 35035 35037 35038 35040 35052\n",
      " 35054 35056 35071 35077 35096 35105 35106 35110 35112 35117 35121 35126\n",
      " 35147 35279 35282 35383 35902 36024 36025 36027 36140 36176 36193 36199\n",
      " 36582 36612 36616 36699 36746 36951 38132 38138 39430 39459 40094 40404\n",
      " 40703 40706 40740 40768 40939 41187 41280 41438 41959 42376 42444 42535\n",
      " 42669 42706 42818 42819 42853 42915 43155 43255 43400 43660 43888 44149\n",
      " 44266 44460 44572 44586] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/root/miniconda3/envs/torch/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "K = 500\n",
    "# 1. 在训练集上拟合特征选择器（非常重要，不要在全数据上fit）\n",
    "selector = SelectKBest(score_func=f_classif, k=K)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# 2. 用相同的选择器 transform 测试集\n",
    "X_test_selected = selector.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
