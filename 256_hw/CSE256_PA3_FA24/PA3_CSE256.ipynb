{"cells":[{"cell_type":"markdown","metadata":{"id":"xuIeelYpumvX"},"source":["# CSE 256 FA24: NLP UCSD PA3:"]},{"cell_type":"markdown","metadata":{"id":"4fwmBoWsumvZ"},"source":["## Retrieval-Augmented Generation (RAG) (40 points)\n","\n","The goal of this assignment is to gain hands-on experience with aspects of **Retrieval-Augmented Generation (RAG)**, with a focus on retrieval. You will use **LangChain**, a framework that simplifies integrating external knowledge into generation tasks by:\n","\n","- Implementing various vector databases for efficient neural retrieval. You will use a vector database for storing our memories.\n","- Allowing seamless integration of pretrained text encoders, which you will access via HuggingFace models. You will use a text encoder to get text embeddings for storing in the vector database.\n","\n","**Data**  \n","You will build a retrieval system using the [QMSum Dataset](https://github.com/Yale-LILY/QMSum), a human-annotated benchmark designed for question answering on long meeting transcripts. The dataset includes over 230 meetings across multiple domains.\n","\n","### <font color='blue'> Release Date: November 6, 2024 | Due Date: November 18, 2024 </font>\n","\n","---\n","\n","**IMPORTANT:** After copying this notebook to your Google Drive along with the two data files, paste a link to your copy below. To create a publicly accessible link:\n","1. Click the *Share* button in the top-right corner.\n","2. Select \"Get shareable link\" and copy the link.\n","\n","#### <font color=\"red\">https://drive.google.com/drive/folders/1jgREfDAJPbBCofiJ_7wSrGwqAXE56k_i?usp=drive_link </font>\n","\n","\n","---\n","**Notes:**\n","\n","Make sure to save the notebook as you go along.\n","\n","Submission instructions are located at the bottom of the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"elapsed":1301,"status":"error","timestamp":1730914780512,"user":{"displayName":"Ndapandula Nakashole","userId":"15450062936262242698"},"user_tz":480},"id":"Fw3Q_WvPvBqA","outputId":"dd471473-11b8-495b-d4e6-26659018b37a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"ename":"AssertionError","evalue":"[!] Enter the foldername.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-7570b900a917>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# e.g. 'CSE156/assignments/PA3/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mFOLDERNAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mFOLDERNAME\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[!] Enter the foldername.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Now that we've mounted your Drive, this ensures that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: [!] Enter the foldername."]}],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# TODO: Enter the foldername in your Drive where you have saved this notebook\n","# e.g. 'CSE156/assignments/PA3/'\n","FOLDERNAME = None\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# This is later used to use the IMDB reviews\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/"]},{"cell_type":"markdown","metadata":{"id":"FP65C9N9umva"},"source":["# RAG Workflow\n","\n","Retrieval-Augmented Generation (RAG) systems involve several interconnected components. Below is a RAG workflow diagram from Hugging Face. Areas highlighted in blue indicate opportunities for system improvement.\n","\n","In this assignment,  we will focus  on the ***Retriever**  so the PA does not cover any processes starting from \"2. Reader\" and below.\n","\n","\n","<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6LcI9x5HE-EJ"},"source":["# First,  install the required model dependancies."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kksG1BIyvhh0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q torch transformers langchain_chroma bitsandbytes langchain faiss-gpu langchain_huggingface langchain-community sentence-transformers  pacmap tqdm matplotlib"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"pRYKFwaOumva"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","import pandas as pd\n","import os\n","import csv\n","import sys\n","import numpy as np\n","import time\n","import random\n","from typing import Optional, List, Tuple\n","import matplotlib.pyplot as plt\n","import textwrap\n","import torch\n","\n","\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","# Disable  huffingface tokenizers parallelism\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"]},{"cell_type":"markdown","metadata":{"id":"b6gIgmDTumvb"},"source":["# Load the meetings dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"OL_vP4Goumvb"},"outputs":[{"ename":"NameError","evalue":"name 'FOLDERNAME' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m documents\n\u001b[1;32m     24\u001b[0m docs \u001b[38;5;241m=\u001b[39m  [] \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m doc_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/My Drive/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/meetings.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mFOLDERNAME\u001b[49m)\n\u001b[1;32m     26\u001b[0m documents \u001b[38;5;241m=\u001b[39m load_documents(doc_file)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc_id \u001b[38;5;129;01min\u001b[39;00m documents:\n","\u001b[0;31mNameError\u001b[0m: name 'FOLDERNAME' is not defined"]}],"source":["from langchain.docstore.document import Document\n","\n","def load_documents(doc_file):\n","    \"\"\"\n","    Loads the document contents from the first file.\n","\n","    :param doc_file: Path to the document file (document ID <TAB> document contents).\n","    :return: A dictionary {document_id: document_contents}.\n","    \"\"\"\n","    max_size = sys.maxsize\n","    csv.field_size_limit(max_size)\n","\n","\n","    documents = {}\n","    with open(doc_file, 'r', encoding='utf-8') as f:\n","        reader = csv.reader(f, delimiter='\\t')\n","        for row in reader:\n","            if len(row)==0: continue\n","            doc_id, content = row\n","            documents[doc_id] = content\n","    return documents\n","\n","\n","docs =  [] #\n","doc_file = '/content/drive/My Drive/{}/meetings.tsv'.format(FOLDERNAME)\n","documents = load_documents(doc_file)\n","\n","for doc_id in documents:\n","    doc = Document(page_content=documents[doc_id])\n","    metadata = {'source': doc_id}\n","    doc.metadata = metadata\n","    docs.append(doc)\n","\n","print(f\"Total meetings (docs):  {len(documents)}\")\n"]},{"cell_type":"markdown","metadata":{"id":"tfFRAwGPumvd"},"source":["# Retriever - Building the retriever üóÇÔ∏è"]},{"cell_type":"markdown","metadata":{"id":"7RzIq2C3umvd"},"source":["The **retriever functions like a search engine**: given a user query, it returns relevant documents from the knowledge base.\n","\n","These documents are then used by the Reader model to generate an answer. In this assignment, however, we are only focusing on the retriever, not the Reader model.\n","\n","**Our goal:** Given a user question, find the most relevant documents from the knowledge base.\n","\n","Key parameters:\n","- `top_k`: The number of documents to retrieve. Increasing `top_k` can improve the chances of retrieving relevant content.\n","- `chunk size`: The length of each document. While this can vary, avoid overly long documents, as too many tokens can overwhelm most reader models.\n","\n","\n","Langchain __offers a huge variety of options for vector databases and allows us to keep document metadata throughout the processing__."]},{"cell_type":"markdown","metadata":{"id":"9m7pdV-Xumvd"},"source":[" ### 1. Specify an Embedding Model and Visualize Document Lengths\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0fGIq3zumvd"},"outputs":[],"source":["EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n","\n","from sentence_transformers import SentenceTransformer\n","\n","print(\n","    f\"Model's maximum sequence length: {SentenceTransformer(EMBEDDING_MODEL_NAME).max_seq_length}\"\n",")\n","\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n","lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs)]\n","\n","# Plot the distribution of document lengths, counted as the number of tokens\n","fig = pd.Series(lengths).hist()\n","plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OgQzRWqNumvd"},"source":["### 2. Split the Documents into Chunks\n","\n","The documents (meeting transcripts) are very long‚Äîsome up to 30,000 tokens! To make retrieval effective, we‚Äôll **split each document into smaller, semantically meaningful chunks**. These chunks will serve as the snippets the retriever compares to the query, returning the `top_k` most relevant ones.\n","\n","**Objective**: Create Semantically Relevant Snippets\n","\n","Chunks should be long enough to capture complete ideas but not so lengthy that they lose focus.\n","\n","We will use Langchain's implementation of recursive chunking with `RecursiveCharacterTextSplitter`.\n","- Parameter `chunk_size` controls the length of individual chunks: this length is counted by default as the number of characters in the chunk.\n","- Parameter `chunk_overlap` lets adjacent chunks get a bit of overlap on each other. This reduces the probability that an idea could be cut in half by the split between two adjacent chunks.\n","\n","From the produced plot below, you can see that now the chunk length distribution looks better!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Ur0Kzz8M5Sj"},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size = 768,\n","    chunk_overlap = 128,\n",")\n","\n","doc_snippets = text_splitter.split_documents(docs)\n","print(f\"Total {len(doc_snippets)} snippets to be stored in our vector store.\")\n","\n","lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(doc_snippets)]\n","\n","# Plot the distribution of document snippet lengths, counted as the number of tokens\n","fig = pd.Series(lengths).hist()\n","plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"qd8GpdZkumve"},"source":["### 3. Build the Vector Database\n","\n","To enable retrieval, we need to compute embeddings for all chunks in our knowledge base. These embeddings will then be stored in a vector database.\n","\n","#### How Retrieval Works\n","\n","A query is embedded using an embedding model and a similarity search finds the closest matching chunks in the vector database.\n","\n","The following cell builds the vector database consisting of  all chunks in our knowledge base.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GUhbaE-umve"},"outputs":[],"source":["from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain_community.vectorstores.utils import DistanceStrategy\n","\n","# Automatically set the device to 'cuda' if available, otherwise use 'cpu'\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Found device: {device}\")\n","\n","\n","embedding_model = HuggingFaceEmbeddings(\n","    model_name=EMBEDDING_MODEL_NAME,\n","    multi_process=True,\n","    model_kwargs={\"device\": device},\n","    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",")\n","\n","start_time = time.time()\n","\n","KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n","    doc_snippets, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",")\n","\n","end_time = time.time()\n","\n","elapsed_time = (end_time - start_time)/60\n","print(f\"Time taken: {elapsed_time} minutes\")\n"]},{"cell_type":"markdown","metadata":{"id":"_lXDDWtDumvf"},"source":["### 4. Querying the Vector Database\n","\n","\n","Using LangChain‚Äôs vector database,  the function `vector_database.similarity_search(query)` implements a Bi-Encoder (covered in class), independently encoding the query and each document into a single-vector representation, allowing document embeddings to be precomputed.\n","\n","Let's  define the Bi-Encoder ranking function and then use it on a sample query from the QMSum dataset.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LgqBmHxBoZRM"},"outputs":[],"source":["## The function for ranking documents given a query:\n","def rank_documents_biencoder(user_query, top_k = 5):\n","    \"\"\"\n","    Function for document ranking based on the query.\n","\n","    :param query: The query to retrieve documents for.\n","    :return: A list of document IDs ranked based on the query (mocked).\n","    \"\"\"\n","    retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=top_k)\n","    ranked_list = []\n","    for i, doc in enumerate(retrieved_docs):\n","        ranked_list.append(retrieved_docs[i].metadata['source'])\n","\n","    return ranked_list  # ranked document IDs.\n","\n","\n","user_query = \"what did kirsty williams am say about her plan for quality assurance ?\"\n","retrieved_docs = rank_documents_biencoder(user_query)\n","\n","print(\"\\n==================================Top-5 documents==================================\")\n","print(\"\\n\\nRetrieved documents:\", retrieved_docs)\n","print(\"\\n====================================================================\\n\")"]},{"cell_type":"markdown","metadata":{"id":"Lpn_GkMhqyU2"},"source":["### <font color=\"red\">5. TODO: Implementation of ColBERT as a Reranker for a Bi-Encoder (35 points)</font>\n","\n","The Bi-Encoder‚Äôs ranking for the sample query is not optimal: the ground truth document is not ranked at position 1, instead the document ID, **doc_211** is ranked at position 1.  To determine the correct document ID for this query, refer to the `questions_answers.tsv` file.\n","\n","In this task, you will implement the [ColBERT](https://arxiv.org/pdf/2004.12832) approach by Khattab and Zaharia. We‚Äôll use a simplified version of ColBERT, focusing on the following key steps:\n","\n","1. Retrieve the top \\( K = 15 \\) documents for query \\( q \\) using the Bi-Encoder.\n","2. Re-rank these top \\( K = 15 \\) documents using ColBERT's fine-grained interaction scoring. This will involve:\n","   - Using frozen BERT embeddings from a HuggingFace BERT model (no training is required, thus our version is not expected to work as well as full-fledged ColBERT).\n","   - Calculating scores based on fine-grained token-level interactions between the query and each document.\n","3. Implement the method `rank_documents_finegrained_interactions()` to perform this re-ranking.\n","   - Test your method on the same query as in the cell from #4 above.\n","   - Print out the entire re-ranked document list of 5 document IDs, as done in  #4 above (the code below does it for you)\n","4. Ensure that your ColBERT implementation ranks the correct document at position 1 for the sample query.\n","\n","\n","***Note***: Since the same document is divided into multiple chunks that retain the original document ID, you may see the same document ID appear multiple times in your top_k results. However, each instance refers to a different chunk of the document's content.\n","\n","***Note2***:  For this PA we are not focused on query latency, just the late interactions part in the ColBERT approach. Thus, we don't have to pre-compute document matrix representations for ColBERT.\n","\n","***Note3:***: Both the bi-encoder and the ColBERT used in this PA are not trained for retrieval and their performance is therefore not SOTA. What we are doing is  ***zero-shot transfer for retrieval from BERT***. For SOTA retrieval performance both have to be trained on data of the form (q, doc+, docs-). For this PA, it is best to leave the setup as is for grading purposes. But you can certainly explore for your own purposes.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"DugEteFxqo8b"},"outputs":[{"name":"stderr","output_type":"stream","text":["/root/miniconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"ename":"NameError","evalue":"name 'KNOWLEDGE_VECTOR_DATABASE' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ranked_list  \u001b[38;5;66;03m# ranked document IDs\u001b[39;00m\n\u001b[1;32m     61\u001b[0m user_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat did kirsty williams am say about her plan for quality assurance ?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 62\u001b[0m retrieved_docs \u001b[38;5;241m=\u001b[39m \u001b[43mrank_documents_finegrained_interactions\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==================================Top-5 documents==================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRetrieved documents:\u001b[39m\u001b[38;5;124m\"\u001b[39m, retrieved_docs)\n","Cell \u001b[0;32mIn[1], line 23\u001b[0m, in \u001b[0;36mrank_documents_finegrained_interactions\u001b[0;34m(user_query, shortlist, top_k)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrank_documents_finegrained_interactions\u001b[39m(user_query, shortlist\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Rerank the top-K=15 retrieved documents from Bi-encoder using fine-grained token-level interactions\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    and return the top_k=5 most similar documents.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    - ranked_list of document IDs.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     retrieved_docs \u001b[38;5;241m=\u001b[39m \u001b[43mKNOWLEDGE_VECTOR_DATABASE\u001b[49m\u001b[38;5;241m.\u001b[39msimilarity_search(query\u001b[38;5;241m=\u001b[39muser_query, k\u001b[38;5;241m=\u001b[39mshortlist)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Tokenize the user query\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     query_inputs \u001b[38;5;241m=\u001b[39m tokenizer(user_query, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mNameError\u001b[0m: name 'KNOWLEDGE_VECTOR_DATABASE' is not defined"]}],"source":["import torch\n","import torch.nn.functional as F\n","from transformers import AutoTokenizer, AutoModel\n","\n","# Load tokenizer and model BERT from HuggingFace\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = AutoModel.from_pretrained(\"bert-base-uncased\")\n","\n","def rank_documents_finegrained_interactions(user_query, shortlist=15, top_k=5):\n","    \"\"\"\n","    Rerank the top-K=15 retrieved documents from Bi-encoder using fine-grained token-level interactions\n","    and return the top_k=5 most similar documents.\n","\n","    Args:\n","    - user_query (str): The user query string.\n","    - shortlist (list): Number of documents in the longer short list\n","    - top_k (int): Number of top reranked documents to return.\n","\n","    Returns:\n","    - ranked_list of document IDs.\n","    \"\"\"\n","\n","    retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=shortlist)\n","\n","    # Tokenize the user query\n","    query_inputs = tokenizer(user_query, return_tensors='pt', truncation=True, padding=True)\n","\n","    # Get query token embeddings from BERT\n","    with torch.no_grad():\n","        query_embeddings = model(**query_inputs).last_hidden_state  # Shape: (1, seq_len_query, hidden_dim)\n","\n","    scores = []\n","\n","    for doc in retrieved_docs:\n","        # Tokenize the document\n","        doc_inputs = tokenizer(doc.page_content, return_tensors='pt', truncation=True, padding=True)\n","\n","        # Get document token embeddings from BERT\n","        with torch.no_grad():\n","            doc_embeddings = model(**doc_inputs).last_hidden_state  # Shape: (1, seq_len_doc, hidden_dim)\n","\n","        # Compute interaction scores\n","        interaction_scores = torch.matmul(query_embeddings, doc_embeddings.transpose(-1, -2))  # Shape: (1, seq_len_query, seq_len_doc)\n","\n","        # Max pooling over document tokens for each query token\n","        max_scores, _ = torch.max(interaction_scores, dim=-1)  # Shape: (1, seq_len_query)\n","\n","        # Aggregate scores (e.g., sum or mean)\n","        doc_score = torch.sum(max_scores).item()\n","\n","        scores.append((doc_score, doc.metadata['source']))\n","\n","    # Sort documents by score in descending order\n","    scores.sort(reverse=True, key=lambda x: x[0])\n","\n","    # Return top_k document IDs\n","    ranked_list = [doc_id for _, doc_id in scores[:top_k]]\n","\n","    return ranked_list  # ranked document IDs\n","\n","user_query = \"what did kirsty williams am say about her plan for quality assurance ?\"\n","retrieved_docs = rank_documents_finegrained_interactions(user_query)\n","\n","print(\"\\n==================================Top-5 documents==================================\")\n","print(\"\\n\\nRetrieved documents:\", retrieved_docs)\n","print(\"\\n====================================================================\\n\")"]},{"cell_type":"markdown","metadata":{"id":"3ZE4pA_8zOfv"},"source":["####<font color=\"red\">6. TODO:  ColBERT Max vs. Mean Pooling for  Relevance Scoring of Documents:</font> (5 points)\n","\n","\n","ColBERT uses a form of **max pooling**, where each query term's contribution to the relevance score of a  document is determined by its maximum similarity to any document term. One alternative approach is **mean pooling**, where each query term's contribution is calculated as the average similarity across all document terms.\n","\n","Discuss  the  merits and potential limitations of using  mean pooling versus max pooling in ColBERT. In your answer, consider how each approach might affect retrieval accuracy, sensitivity to specific token matches,  interpretability, and anything you deem relevant. You are welcome to use an analysis of  ColBERT's  performance on the provided sample query in your discussion, but this is not required.\n"]},{"cell_type":"markdown","metadata":{"id":"u9Ql0nN80bTN"},"source":["#### <font color=\"red\">Explain here (<= 5 sentences) : </font>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4XCJrSWg0Zia"},"source":["----------------------------\n","# <font color=\"blue\"> Submission Instructions</font>\n","---------------------------\n","\n","1. Click the Save button at the top of the Jupyter Notebook.\n","2. Select Runtime -> Run All. This will run all the cells in order, and will take several minutes.\n","3. Once you've rerun everything,  save a PDF version of your notebook. Make sure all your code and answers  are displayed in the pdf</font>, it's okay if the provided codes get cut off because lines are not wrapped in code cells).\n","4. Look at the PDF file and make sure all your code and answers  are there, displayed correctly. The PDF is the only thing your graders will see!\n","5. Submit your PDF on Gradescope."]},{"cell_type":"markdown","metadata":{"id":"vgLja6qt0-To"},"source":["### <font color=\"green\">  7.  (Optional) Full evaluation pipeline for your own exploration. </font>\n","\n","\n","For this assignment, we only ask you to explore  one  sample query.\n","Running on many queries is super slow without the right compute.\n","If you have compute/and/or time to wait, below is a more complete evaluation setup that works with all the queries in QMSum dataset, and  reports the  `precision@k=5` metric.\n","\n","\n","**Note**: you need to remove the comment markers from the code below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0pjYhz1umvj"},"outputs":[],"source":["\n","# def load_questions_answers(qa_file):\n","#     \"\"\"\n","#     Loads the questions and corresponding ground truth document IDs.\n","\n","#     :param qa_file: Path to the question-answer file (document ID <TAB> question <TAB> answer).\n","#     :return: A list of tuples [(document_id, question, answer)].\n","#     \"\"\"\n","#     qa_pairs = []\n","#     with open(qa_file, 'r', encoding='utf-8') as f:\n","#         reader = csv.reader(f, delimiter='\\t')\n","#         for row in reader:\n","#             doc_id, question, answer = row\n","#             qa_pairs.append((doc_id, question, answer))\n","\n","#     random.shuffle(qa_pairs)\n","\n","#     return qa_pairs\n","\n","# def precision_at_k(ground_truth, retrieved_docs, k):\n","#     \"\"\"\n","#     Computes Precision at k for a single query.\n","\n","#     :param ground_truth: The name of the ground truth document.\n","#     :param retrieved_docs: The list of document names returned by the model in ranked order.\n","#     :param k: The cutoff for computing Precision.\n","#     :return: Precision at k.\n","#     \"\"\"\n","#     return 1 if ground_truth in retrieved_docs[:k] else 0\n","\n","# def evaluate(doc_file, qa_file, ranking_fuction = None, k= 5):\n","#     \"\"\"\n","#     Evaluate the retrieval system based on the documents and question-answer pairs.\n","\n","#     :param doc_file: Path to the document file.\n","#     :param qa_file: Path to the question-answer file.\n","#     :param k: The cutoff for Precision@k.\n","#     \"\"\"\n","#     # Load the QA pairs\n","#     qa_pairs = load_questions_answers(qa_file)\n","\n","#     precision_scores = []\n","\n","\n","#     for doc_id, question, _ in qa_pairs:\n","\n","#         retrieved_docs = ranking_fuction(question)\n","#         precision_scores.append(precision_at_k(doc_id, retrieved_docs, k))\n","\n","#         avg_precision_at_k = sum(precision_scores) / len(precision_scores)\n","\n","#         if len(precision_scores) %10==0:\n","#             print(f\"After {len(precision_scores)} queries, Precision@{k}: {avg_precision_at_k}\")\n","\n","#     # Compute average Precision@k\n","#     avg_precision_at_k = sum(precision_scores) / len(precision_scores)\n","\n","#     print(f\"Precision@{k}: {avg_precision_at_k}\")\n","\n","\n","# qa_file = 'questions_answers.tsv'  # document ID <TAB> question <TAB> answer\n","\n","# start_time = time.time()\n","# evaluate(doc_file, qa_file,rank_documents_biencoder)\n","# end_time = time.time()\n","# elapsed_time = (end_time - start_time)/60\n","# print(f\"Time taken: {elapsed_time} minutes\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"}},"nbformat":4,"nbformat_minor":0}
