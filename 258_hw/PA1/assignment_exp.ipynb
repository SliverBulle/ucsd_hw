{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "\n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization_rating_prediction(ratingsTrain, ratingsValid, n_factors=50, reg=0.1):\n",
    "    # 构建用户和物品的索引映射\n",
    "    users = list(set(u for u, _, _ in ratingsTrain))\n",
    "    items = list(set(b for _, b, _ in ratingsTrain))\n",
    "    user_to_idx = {u: i for i, u in enumerate(users)}\n",
    "    item_to_idx = {b: i for i, b in enumerate(items)}\n",
    "    \n",
    "    n_users = len(users)\n",
    "    n_items = len(items)\n",
    "    \n",
    "    # 构建评分矩阵\n",
    "    R = np.zeros((n_users, n_items))\n",
    "    for u, b, r in ratingsTrain:\n",
    "        try:\n",
    "            R[user_to_idx[u], item_to_idx[b]] = float(r)  # 转换为浮点数\n",
    "        except ValueError:\n",
    "            R[user_to_idx[u], item_to_idx[b]] = 0.0  # 处理无法转换的情况\n",
    "    \n",
    "    # 使用SVD进行矩阵分解\n",
    "    svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "    U = svd.fit_transform(R)\n",
    "    Sigma = svd.singular_values_\n",
    "    Vt = svd.components_\n",
    "    \n",
    "    # 预测评分\n",
    "    R_pred = np.dot(U, np.dot(np.diag(Sigma), Vt))\n",
    "    \n",
    "    # 计算验证集的MSE\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for u, b, r in ratingsValid:\n",
    "        if u in user_to_idx and b in item_to_idx:\n",
    "            pred = R_pred[user_to_idx[u], item_to_idx[b]]\n",
    "        else:\n",
    "            pred = np.mean(R[R > 0])  # 使用全局平均评分\n",
    "        try:\n",
    "            y_true.append(float(r))  # 转换为浮点数\n",
    "        except ValueError:\n",
    "            y_true.append(np.mean(R[R > 0]))  # 无法转换时使用全局平均评分\n",
    "        y_pred.append(pred)\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    print(f\"Validation MSE: {mse}\")\n",
    "    return mse, R_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_read_prediction(ratingsTrain, return1, ratingsPerUser, usersPerItem):\n",
    "    # 构建特征和标签\n",
    "    features = []\n",
    "    labels = []\n",
    "    user_avg_rating = defaultdict(float)\n",
    "    item_count = defaultdict(int)\n",
    "    user_interaction_count = defaultdict(int)\n",
    "    \n",
    "    # 首先计算每个用户的总评分和交互次数\n",
    "    for u, b, r in ratingsTrain:\n",
    "        try:\n",
    "            user_avg_rating[u] += float(r)\n",
    "            user_interaction_count[u] += 1\n",
    "            item_count[b] += 1\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "    \n",
    "    # 计算平均评分，避免除以零\n",
    "    for u in user_avg_rating:\n",
    "        if user_interaction_count[u] > 0:  # 只有当有交互时才计算平均值\n",
    "            user_avg_rating[u] /= user_interaction_count[u]\n",
    "        else:\n",
    "            user_avg_rating[u] = 0.0  # 如果没有交互，设置默认值为0\n",
    "\n",
    "    for u, b, r in tqdm(ratingsTrain):\n",
    "        feature = []\n",
    "        # 特征1：用户平均评分\n",
    "        feature.append(user_avg_rating[u])\n",
    "        \n",
    "        # 特征2：物品被评分次数\n",
    "        feature.append(item_count[b])\n",
    "        \n",
    "        # 特征3：用户-物品互动次数\n",
    "        interaction_count = len([1 for _, item, _ in ratingsTrain if item == b and _ == u])\n",
    "        feature.append(interaction_count)\n",
    "        \n",
    "        # 特征4：是否在热门物品中\n",
    "        feature.append(1 if b in return1 else 0)\n",
    "        \n",
    "        # 特征5：用户与物品的Jaccard相似度\n",
    "        user_books = ratingsPerUser[u]\n",
    "        max_sim = max([jaccard_similarity(set(usersPerItem[b]), set(usersPerItem[read_book])) for read_book in user_books], default=0)\n",
    "        feature.append(1 if max_sim > 0.01 else 0)\n",
    "        \n",
    "        features.append(feature)\n",
    "        labels.append(1 if int(r) > 0 else 0)\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(s1, s2):\n",
    "    intersection = len(s1 & s2)\n",
    "    union = len(s1 | s2)\n",
    "    return intersection / union if union != 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_read_prediction(features, labels):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(features, labels, test_size=0.1, random_state=42)\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=100,\n",
    "                    valid_sets=[lgb_train, lgb_valid])\n",
    "    \n",
    "    y_pred = gbm.predict(X_valid, num_iteration=gbm.best_iteration)\n",
    "    y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "    accuracy = accuracy_score(y_valid, y_pred_binary)\n",
    "    print(f\"Validation Accuracy: {accuracy}\")\n",
    "    return gbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bias_model_sklearn_sparse(train_data, valid_data, lambda_reg=1.0):\n",
    "    # 构建用户和物品的索引映射\n",
    "    users = list(set(u for u,_,_ in train_data))\n",
    "    items = list(set(b for _,b,_ in train_data))\n",
    "    user_to_idx = {u:i for i,u in enumerate(users)}\n",
    "    item_to_idx = {b:i for i,b in enumerate(items)}\n",
    "    \n",
    "    n_users = len(users)\n",
    "    n_items = len(items)\n",
    "    \n",
    "    # 使用稀疏矩阵构建训练数据\n",
    "    X_train = lil_matrix((len(train_data), n_users + n_items))\n",
    "    y_train = np.zeros(len(train_data))\n",
    "    \n",
    "    for i, (u,b,r) in enumerate(train_data):\n",
    "        X_train[i, user_to_idx[u]] = 1  # 用户one-hot编码\n",
    "        X_train[i, n_users + item_to_idx[b]] = 1  # 物品one-hot编码\n",
    "        y_train[i] = r\n",
    "    \n",
    "    # 训练模型\n",
    "    model = Ridge(alpha=lambda_reg, fit_intercept=True, solver='sag')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 使用稀疏矩阵构建验证数据\n",
    "    X_valid = lil_matrix((len(valid_data), n_users + n_items))\n",
    "    y_valid = np.zeros(len(valid_data))\n",
    "    \n",
    "    for i, (u,b,r) in enumerate(valid_data):\n",
    "        if u in user_to_idx and b in item_to_idx:\n",
    "            X_valid[i, user_to_idx[u]] = 1\n",
    "            X_valid[i, n_users + item_to_idx[b]] = 1\n",
    "        y_valid[i] = r\n",
    "    \n",
    "    y_pred = model.predict(X_valid)\n",
    "    valid_mse = mean_squared_error(y_valid, y_pred)\n",
    "    \n",
    "    return valid_mse, model.intercept_, model.coef_[:n_users], model.coef_[n_users:], model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings start\n",
      "开始评分预测模型训练...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "print(\"ratings start\")\n",
    "allRatings = []\n",
    "userRatings = defaultdict(list)\n",
    "\n",
    "for l in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    allRatings.append(l)\n",
    "\n",
    "ratingsTrain = allRatings[:190000]\n",
    "ratingsValid = allRatings[190000:]\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "\n",
    "for u, b, r in ratingsTrain:\n",
    "    ratingsPerUser[u].append(b)\n",
    "    ratingsPerItem[b].append((u, int(r)))\n",
    "    usersPerItem[b].add(u)\n",
    "    itemsPerUser[u].add(b)\n",
    "\n",
    "# 计算最受欢迎的书籍\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user, book, _ in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = sorted([(count, book) for book, count in bookCount.items()], reverse=True)\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalRead * 0.5:\n",
    "        break\n",
    "\n",
    "# 评分预测\n",
    "print(\"开始评分预测模型训练...\")\n",
    "#mse, R_pred = matrix_factorization_rating_prediction(ratingsTrain, ratingsValid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始阅读预测特征工程...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190000/190000 [16:06<00:00, 196.64it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 阅读预测\n",
    "print(\"开始阅读预测特征工程...\")\n",
    "features, labels = feature_engineering_read_prediction(ratingsTrain, return1, ratingsPerUser, usersPerItem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练梯度提升决策树模型...\n",
      "[LightGBM] [Info] Number of positive: 161795, number of negative: 9205\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 460\n",
      "[LightGBM] [Info] Number of data points in the train set: 171000, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.946170 -> initscore=2.866583\n",
      "[LightGBM] [Info] Start training from score 2.866583\n",
      "Validation Accuracy: 0.9552631578947368\n"
     ]
    }
   ],
   "source": [
    "print(\"开始训练梯度提升决策树模型...\")\n",
    "gbm = gradient_boosting_read_prediction(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始进行评分预测...\n",
      "评分预测完成，结果已保存到 predictions_Rating.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 评分预测示例\n",
    "print(\"开始进行评分预测...\")\n",
    "users = list(set(u for u, _, _ in ratingsTrain))\n",
    "items = list(set(b for _, b, _ in ratingsTrain))\n",
    "user_to_idx = {u:i for i, u in enumerate(users)}\n",
    "item_to_idx = {b:i for i, b in enumerate(items)}\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit([[u,b] for u, b, _ in ratingsTrain])\n",
    "\n",
    "model_rating = Ridge(alpha=4.6, fit_intercept=True, solver='sag')\n",
    "X_train_rating = encoder.transform([[u, b] for u, b, _ in ratingsTrain])\n",
    "y_train_rating = np.array([r for _, _, r in ratingsTrain])\n",
    "model_rating.fit(X_train_rating, y_train_rating)\n",
    "\n",
    "with open(\"predictions_Rating.csv\", 'w') as predictions_rating:\n",
    "    for l in open(\"pairs_Rating.csv\"):\n",
    "        if l.startswith(\"userID\"):\n",
    "            predictions_rating.write(l)\n",
    "            continue\n",
    "        u, b = l.strip().split(',')\n",
    "        \n",
    "        try:\n",
    "            # 使用与训练时相同的encoder进行转换\n",
    "            feature_vector = encoder.transform([[u, b]])\n",
    "            prediction = model_rating.predict(feature_vector)[0]\n",
    "        except:\n",
    "            # 处理未知用户或书籍的情况\n",
    "            prediction = 3  # 使用默认评分\n",
    "        \n",
    "        # 确保评分在合理范围内\n",
    "        prediction = max(1, min(5, prediction))\n",
    "        \n",
    "        predictions_rating.write(f\"{u},{b},{int(round(prediction))}\\n\")\n",
    "\n",
    "print(\"评分预测完成，结果已保存到 predictions_Rating.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始进行评分预测...\n",
      "计算统计信息...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting ratings:  88%|████████▊ | 176404/200000 [00:00<00:00, 885551.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting ratings: 100%|██████████| 200000/200000 [00:00<00:00, 894411.82it/s]\n",
      "Computing user stats: 27943it [00:00, 40503.13it/s]\n",
      "Computing item stats: 6688it [00:00, 22381.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备训练数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating features: 200000it [00:02, 82573.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练模型...\n",
      "开始预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ratings: 100%|██████████| 10000/10000 [00:01<00:00, 6847.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评分预测完成，结果已保存到 predictions_Rating.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "\n",
    "print(\"开始进行评分预测...\")\n",
    "\n",
    "# 1. 预计算统计信息\n",
    "print(\"计算统计信息...\")\n",
    "user_stats = {}\n",
    "item_stats = {}\n",
    "all_ratings = []\n",
    "\n",
    "# 首先收集所有有效评分\n",
    "for u, b, r in tqdm(allRatings, desc=\"Collecting ratings\"):\n",
    "    try:\n",
    "        rating = float(r)\n",
    "        if not np.isnan(rating):  # 确保评分是有效的数字\n",
    "            all_ratings.append((u, b, rating))\n",
    "    except (ValueError, TypeError):\n",
    "        continue\n",
    "\n",
    "# 转换为numpy数组\n",
    "ratings_array = np.array(all_ratings)\n",
    "users = ratings_array[:, 0]\n",
    "items = ratings_array[:, 1]\n",
    "ratings = ratings_array[:, 2].astype(float)  # 确保评分是float类型\n",
    "\n",
    "global_mean = np.mean(ratings)\n",
    "\n",
    "# 计算用户统计信息\n",
    "for u, g in tqdm(groupby(sorted(zip(users, ratings)), key=lambda x: x[0]), desc=\"Computing user stats\"):\n",
    "    user_ratings = np.array([float(r) for _, r in g])\n",
    "    user_stats[u] = {\n",
    "        'mean': np.mean(user_ratings),\n",
    "        'std': np.std(user_ratings) if len(user_ratings) > 1 else 0,\n",
    "        'count': len(user_ratings)\n",
    "    }\n",
    "\n",
    "# 计算物品统计信息\n",
    "for b, g in tqdm(groupby(sorted(zip(items, ratings)), key=lambda x: x[0]), desc=\"Computing item stats\"):\n",
    "    item_ratings = np.array([float(r) for _, r in g])\n",
    "    item_stats[b] = {\n",
    "        'mean': np.mean(item_ratings),\n",
    "        'std': np.std(item_ratings) if len(item_ratings) > 1 else 0,\n",
    "        'count': len(item_ratings)\n",
    "    }\n",
    "\n",
    "# 2. 基线预测模型优化\n",
    "def baseline_predictor(user, item):\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        \n",
    "        # 动态权重调整\n",
    "        u_weight = 1 / (1 + np.exp(-u_stats['count']/10))  # sigmoid函数\n",
    "        i_weight = 1 / (1 + np.exp(-i_stats['count']/10))\n",
    "        \n",
    "        # 考虑评分标准差\n",
    "        u_reliability = 1 / (1 + u_stats['std'])\n",
    "        i_reliability = 1 / (1 + i_stats['std'])\n",
    "        \n",
    "        # 组合预测\n",
    "        weights_sum = u_weight * u_reliability + i_weight * i_reliability\n",
    "        if weights_sum > 0:\n",
    "            base_pred = (u_weight * u_reliability * u_stats['mean'] + \n",
    "                        i_weight * i_reliability * i_stats['mean']) / weights_sum\n",
    "        else:\n",
    "            base_pred = global_mean\n",
    "    elif user in user_stats:\n",
    "        base_pred = 0.7 * user_stats[user]['mean'] + 0.3 * global_mean\n",
    "    elif item in item_stats:\n",
    "        base_pred = 0.7 * item_stats[item]['mean'] + 0.3 * global_mean\n",
    "    else:\n",
    "        base_pred = global_mean\n",
    "    return base_pred\n",
    "\n",
    "# 3. 增强特征工程\n",
    "def create_features(user, item):\n",
    "    u_stats = user_stats.get(user, {'mean': global_mean, 'std': 0, 'count': 0})\n",
    "    i_stats = item_stats.get(item, {'mean': global_mean, 'std': 0, 'count': 0})\n",
    "    baseline = baseline_predictor(user, item)\n",
    "    \n",
    "    # 基础特征\n",
    "    features = [\n",
    "        u_stats['mean'] - global_mean,     # 用户评分偏差\n",
    "        i_stats['mean'] - global_mean,     # 物品评分偏差\n",
    "        np.log1p(u_stats['count']),        # 用户评分数量（对数）\n",
    "        np.log1p(i_stats['count']),        # 物品评分数量（对数）\n",
    "        baseline - global_mean,            # 基线预测偏差\n",
    "        u_stats['std'],                    # 用户标准差\n",
    "        i_stats['std'],                    # 物品标准差\n",
    "    ]\n",
    "    \n",
    "    # 交互特征\n",
    "    features.extend([\n",
    "        (u_stats['mean'] * i_stats['mean']) / 25,  # 均值交互\n",
    "        abs(u_stats['mean'] - i_stats['mean']),    # 均值差异\n",
    "        u_stats['std'] * i_stats['std'],           # 标准差交互\n",
    "        np.log1p(u_stats['count']) * np.log1p(i_stats['count']), # 评分数量交互\n",
    "        1 / (1 + u_stats['std']),         # 用户可靠性\n",
    "        1 / (1 + i_stats['std']),         # 物品可靠性\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 4. 模型训练（使用交叉验证选择最佳参数）\n",
    "print(\"准备训练数据...\")\n",
    "X_train = np.array([create_features(u, b) for u, b, r in tqdm(zip(users, items, ratings), desc=\"Creating features\")])\n",
    "y_train = ratings - global_mean\n",
    "\n",
    "# 标准化特征\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# 使用ElasticNet而不是Ridge\n",
    "print(\"训练模型...\")\n",
    "from sklearn.linear_model import ElasticNet\n",
    "model = ElasticNet(\n",
    "    alpha=0.001,  # 较小的正则化参数\n",
    "    l1_ratio=0.1, # 主要使用L2正则化\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 5. 预测\n",
    "print(\"开始预测...\")\n",
    "with open(\"predictions_Rating.csv\", 'w') as predictions_rating:\n",
    "    predictions_rating.write(\"userID,bookID,rating\\n\")\n",
    "    \n",
    "    test_pairs = []\n",
    "    with open(\"pairs_Rating.csv\") as f:\n",
    "        next(f)\n",
    "        for l in f:\n",
    "            test_pairs.append(l.strip().split(','))\n",
    "    \n",
    "    for u, b in tqdm(test_pairs, desc=\"Predicting ratings\"):\n",
    "        try:\n",
    "            features = create_features(u, b)\n",
    "            features_scaled = scaler.transform([features])\n",
    "            pred_diff = model.predict(features_scaled)[0]\n",
    "            prediction = global_mean + pred_diff\n",
    "            \n",
    "            # 智能范围限制\n",
    "            u_stats = user_stats.get(u, {'std': 1.0, 'mean': global_mean})\n",
    "            i_stats = item_stats.get(b, {'std': 1.0, 'mean': global_mean})\n",
    "            \n",
    "            # 自适应预测范围\n",
    "            base = baseline_predictor(u, b)\n",
    "            confidence = 1 / (1 + u_stats['std'] + i_stats['std'])  # 可信度\n",
    "            allowed_diff = (2.0 - confidence) * 1.5  # 可信度越高，允许的差异越小\n",
    "            \n",
    "            # 加权组合\n",
    "            w = 0.7  # 模型预测的权重\n",
    "            prediction = w * prediction + (1-w) * base\n",
    "            prediction = max(base - allowed_diff, min(base + allowed_diff, prediction))\n",
    "            \n",
    "        except:\n",
    "            prediction = baseline_predictor(u, b)\n",
    "        \n",
    "        prediction = max(1, min(5, prediction))\n",
    "        predictions_rating.write(f\"{u},{b},{int(round(prediction))}\\n\")\n",
    "\n",
    "print(\"评分预测完成，结果已保存到 predictions_Rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始进行评分预测...\n",
      "计算统计信息...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting ratings: 100%|██████████| 200000/200000 [00:00<00:00, 784379.38it/s]\n",
      "Computing user stats: 27943it [00:00, 38952.26it/s]\n",
      "Computing item stats: 6688it [00:00, 22944.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建评分矩阵...\n",
      "训练SVD模型...\n",
      "开始预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ratings: 100%|██████████| 10000/10000 [00:00<00:00, 62799.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评分预测完成，结果已保存到 predictions_Rating.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"开始进行评分预测...\")\n",
    "\n",
    "# 1. 预计算统计信息\n",
    "print(\"计算统计信息...\")\n",
    "user_stats = {}\n",
    "item_stats = {}\n",
    "all_ratings = []\n",
    "\n",
    "# 首先收集所有有效评分\n",
    "for u, b, r in tqdm(allRatings, desc=\"Collecting ratings\"):\n",
    "    try:\n",
    "        rating = float(r)\n",
    "        if not np.isnan(rating):  # 确保评分是有效的数字\n",
    "            all_ratings.append((u, b, rating))\n",
    "    except (ValueError, TypeError):\n",
    "        continue\n",
    "\n",
    "# 转换为numpy数组\n",
    "ratings_array = np.array(all_ratings)\n",
    "users = ratings_array[:, 0]\n",
    "items = ratings_array[:, 1]\n",
    "ratings = ratings_array[:, 2].astype(float)  # 确保评分是float类型\n",
    "\n",
    "global_mean = np.mean(ratings)\n",
    "\n",
    "# 计算用户统计信息\n",
    "for u, g in tqdm(groupby(sorted(zip(users, ratings)), key=lambda x: x[0]), desc=\"Computing user stats\"):\n",
    "    user_ratings = np.array([float(r) for _, r in g])\n",
    "    user_stats[u] = {\n",
    "        'mean': np.mean(user_ratings),\n",
    "        'std': np.std(user_ratings) if len(user_ratings) > 1 else 0,\n",
    "        'count': len(user_ratings)\n",
    "    }\n",
    "\n",
    "# 计算物品统计信息\n",
    "for b, g in tqdm(groupby(sorted(zip(items, ratings)), key=lambda x: x[0]), desc=\"Computing item stats\"):\n",
    "    item_ratings = np.array([float(r) for _, r in g])\n",
    "    item_stats[b] = {\n",
    "        'mean': np.mean(item_ratings),\n",
    "        'std': np.std(item_ratings) if len(item_ratings) > 1 else 0,\n",
    "        'count': len(item_ratings)\n",
    "    }\n",
    "# 2. 构建稀疏评分矩阵\n",
    "print(\"构建评分矩阵...\")\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# 创建用户和物品的索引映射\n",
    "unique_users = sorted(list(user_stats.keys()))\n",
    "unique_items = sorted(list(item_stats.keys()))\n",
    "user_to_idx = {u: i for i, u in enumerate(unique_users)}\n",
    "item_to_idx = {i: j for j, i in enumerate(unique_items)}\n",
    "\n",
    "# 构建稀疏矩阵\n",
    "row = []\n",
    "col = []\n",
    "data = []\n",
    "for u, b, r in zip(users, items, ratings):\n",
    "    if u in user_to_idx and b in item_to_idx:\n",
    "        row.append(user_to_idx[u])\n",
    "        col.append(item_to_idx[b])\n",
    "        # 去除全局和用户/物品偏差\n",
    "        adj_rating = float(r) - global_mean\n",
    "        adj_rating -= (user_stats[u]['mean'] - global_mean)\n",
    "        adj_rating -= (item_stats[b]['mean'] - global_mean)\n",
    "        data.append(adj_rating)\n",
    "\n",
    "R = csr_matrix((data, (row, col)), shape=(len(unique_users), len(unique_items)))\n",
    "\n",
    "# 3. 使用截断SVD学习隐含特征\n",
    "print(\"训练SVD模型...\")\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "n_factors = 30  # 隐含特征数量\n",
    "svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "user_factors = svd.fit_transform(R)\n",
    "item_factors = svd.components_\n",
    "\n",
    "# 4. 基线预测模型\n",
    "def baseline_predictor(user, item):\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        \n",
    "        # 计算可靠性权重\n",
    "        u_conf = np.tanh(u_stats['count'] / 10)  # 使用tanh函数\n",
    "        i_conf = np.tanh(i_stats['count'] / 10)\n",
    "        \n",
    "        # 计算偏差\n",
    "        u_bias = (u_stats['mean'] - global_mean) * u_conf\n",
    "        i_bias = (i_stats['mean'] - global_mean) * i_conf\n",
    "        \n",
    "        # 加权预测\n",
    "        pred = global_mean + 0.7 * u_bias + 0.7 * i_bias\n",
    "        \n",
    "        # 如果可以，添加矩阵分解预测\n",
    "        try:\n",
    "            u_idx = user_to_idx.get(user)\n",
    "            i_idx = item_to_idx.get(item)\n",
    "            if u_idx is not None and i_idx is not None:\n",
    "                mf_pred = np.dot(user_factors[u_idx], item_factors[:, i_idx])\n",
    "                # 根据用户和物品的评分数量调整矩阵分解的权重\n",
    "                mf_weight = min(0.3, (u_conf + i_conf) / 4)\n",
    "                pred = (1 - mf_weight) * pred + mf_weight * (mf_pred + global_mean)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return pred\n",
    "    elif user in user_stats:\n",
    "        return 0.8 * user_stats[user]['mean'] + 0.2 * global_mean\n",
    "    elif item in item_stats:\n",
    "        return 0.8 * item_stats[item]['mean'] + 0.2 * global_mean\n",
    "    return global_mean\n",
    "\n",
    "# 5. 预测\n",
    "print(\"开始预测...\")\n",
    "with open(\"predictions_Rating.csv\", 'w') as predictions_rating:\n",
    "    predictions_rating.write(\"userID,bookID,rating\\n\")\n",
    "    \n",
    "    test_pairs = []\n",
    "    with open(\"pairs_Rating.csv\") as f:\n",
    "        next(f)\n",
    "        for l in f:\n",
    "            test_pairs.append(l.strip().split(','))\n",
    "    \n",
    "    for u, b in tqdm(test_pairs, desc=\"Predicting ratings\"):\n",
    "        try:\n",
    "            # 获取基线预测\n",
    "            base_pred = baseline_predictor(u, b)\n",
    "            \n",
    "            # 获取用户和物品的统计信息\n",
    "            u_stats = user_stats.get(u, {'std': 1.0, 'mean': global_mean, 'count': 0})\n",
    "            i_stats = item_stats.get(b, {'std': 1.0, 'mean': global_mean, 'count': 0})\n",
    "            \n",
    "            # 计算可信度\n",
    "            u_conf = np.tanh(u_stats['count'] / 10)\n",
    "            i_conf = np.tanh(i_stats['count'] / 10)\n",
    "            reliability = (u_conf + i_conf) / 2\n",
    "            \n",
    "            # 根据可信度调整预测范围\n",
    "            allowed_diff = 2.0 * (1 - reliability)\n",
    "            prediction = base_pred\n",
    "            \n",
    "            # 确保预测在合理范围内\n",
    "            prediction = max(base_pred - allowed_diff, min(base_pred + allowed_diff, prediction))\n",
    "            prediction = max(1, min(5, prediction))\n",
    "            \n",
    "        except:\n",
    "            prediction = global_mean\n",
    "            prediction = max(1, min(5, prediction))\n",
    "        \n",
    "        predictions_rating.write(f\"{u},{b},{int(round(prediction))}\\n\")\n",
    "\n",
    "print(\"评分预测完成，结果已保存到 predictions_Rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始进行阅读预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/torch/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/root/miniconda3/envs/torch/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阅读预测完成，结果已保存到 predictions_Read.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 阅读预测示例\n",
    "print(\"开始进行阅读预测...\")\n",
    "# 构建测试集特征\n",
    "test_features = []\n",
    "test_users = []\n",
    "test_books = []\n",
    "\n",
    "with open(\"pairs_Read.csv\", 'r') as test_file:\n",
    "    for l in test_file:\n",
    "        if l.startswith(\"userID\"):\n",
    "            continue\n",
    "        u, b = l.strip().split(',')\n",
    "        feature = []\n",
    "        # 特征1：用户平均评分\n",
    "        user_avg = np.mean([r for _, _, r in ratingsTrain if _ == u]) if u in ratingsPerUser else 0\n",
    "        feature.append(user_avg)\n",
    "        \n",
    "        # 特征2：物品被评分次数\n",
    "        item_cnt = len(ratingsPerItem[b]) if b in ratingsPerItem else 0\n",
    "        feature.append(item_cnt)\n",
    "        \n",
    "        # 特征3：用户-物品互动次数\n",
    "        interaction_cnt = len([1 for _, item, _ in ratingsTrain if item == b and _ == u]) if u in ratingsPerUser else 0\n",
    "        feature.append(interaction_cnt)\n",
    "        \n",
    "        # 特征4：是否在热门物品中\n",
    "        feature.append(1 if b in return1 else 0)\n",
    "        \n",
    "        # 特征5：用户与物品的Jaccard相似度\n",
    "        if u in ratingsPerUser and b in usersPerItem:\n",
    "            user_books = set(ratingsPerUser[u])\n",
    "            sim = max([jaccard_similarity(usersPerItem[b], usersPerItem[read_book]) for read_book in user_books], default=0)\n",
    "            feature.append(1 if sim > 0.01 else 0)\n",
    "        else:\n",
    "            feature.append(0)\n",
    "        \n",
    "        test_features.append(feature)\n",
    "        test_users.append(u)\n",
    "        test_books.append(b)\n",
    "\n",
    "test_features = np.array(test_features)\n",
    "read_predictions = gbm.predict(test_features)\n",
    "read_predictions_binary = [1 if pred > 0.5 else 0 for pred in read_predictions]\n",
    "\n",
    "with open(\"predictions_Read.csv\", 'w') as predictions_read:\n",
    "    predictions_read.write(\"userID,bookID,prediction\\n\")\n",
    "    for u, b, pred in zip(test_users, test_books, read_predictions_binary):\n",
    "        predictions_read.write(f\"{u},{b},{pred}\\n\")\n",
    "\n",
    "print(\"阅读预测完成，结果已保存到 predictions_Read.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ratings: 100%|██████████| 10000/10000 [00:00<00:00, 74126.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评分预测完成，结果已保存到 predictions_Rating.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. 多个基线预测器\n",
    "def baseline1(user, item):\n",
    "    \"\"\"基于加权平均的基线预测器\"\"\"\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        w_u = np.tanh(u_stats['count'] / 10)\n",
    "        w_i = np.tanh(i_stats['count'] / 10)\n",
    "        pred = global_mean + \\\n",
    "               w_u * (u_stats['mean'] - global_mean) + \\\n",
    "               w_i * (i_stats['mean'] - global_mean)\n",
    "        return pred\n",
    "    elif user in user_stats:\n",
    "        return 0.8 * user_stats[user]['mean'] + 0.2 * global_mean\n",
    "    elif item in item_stats:\n",
    "        return 0.8 * item_stats[item]['mean'] + 0.2 * global_mean\n",
    "    return global_mean\n",
    "\n",
    "def baseline2(user, item):\n",
    "    \"\"\"基于可靠性的基线预测器\"\"\"\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        u_reliability = 1 / (1 + u_stats['std'])\n",
    "        i_reliability = 1 / (1 + i_stats['std'])\n",
    "        weights_sum = u_reliability + i_reliability\n",
    "        if weights_sum > 0:\n",
    "            pred = (u_reliability * u_stats['mean'] + \n",
    "                   i_reliability * i_stats['mean']) / weights_sum\n",
    "            return 0.8 * pred + 0.2 * global_mean\n",
    "    return global_mean\n",
    "\n",
    "def baseline3(user, item):\n",
    "    \"\"\"基于评分数量的指数衰减预测器\"\"\"\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        w_u = 1 - np.exp(-u_stats['count'] / 20)\n",
    "        w_i = 1 - np.exp(-i_stats['count'] / 20)\n",
    "        pred = ((w_u * u_stats['mean'] + w_i * i_stats['mean']) / \n",
    "                (w_u + w_i) if (w_u + w_i) > 0 else global_mean)\n",
    "        return pred\n",
    "    return global_mean\n",
    "\n",
    "# 3. 预测集成\n",
    "def ensemble_predict(user, item):\n",
    "    # 获取各个基线预测\n",
    "    pred1 = baseline1(user, item)\n",
    "    pred2 = baseline2(user, item)\n",
    "    pred3 = baseline3(user, item)\n",
    "    \n",
    "    # 获取用户和物品的统计信息\n",
    "    u_stats = user_stats.get(user, {'std': 1.0, 'mean': global_mean, 'count': 0})\n",
    "    i_stats = item_stats.get(item, {'std': 1.0, 'mean': global_mean, 'count': 0})\n",
    "    \n",
    "    # 计算可信度\n",
    "    u_conf = np.tanh(u_stats['count'] / 10)\n",
    "    i_conf = np.tanh(i_stats['count'] / 10)\n",
    "    reliability = (u_conf + i_conf) / 2\n",
    "    \n",
    "    # 根据可信度调整权重\n",
    "    if reliability > 0.8:\n",
    "        # 高可信度时更信任baseline1\n",
    "        weights = [0.5, 0.25, 0.25]\n",
    "    elif reliability > 0.5:\n",
    "        # 中等可信度时平均权重\n",
    "        weights = [0.4, 0.3, 0.3]\n",
    "    else:\n",
    "        # 低可信度时更信任baseline2和baseline3\n",
    "        weights = [0.2, 0.4, 0.4]\n",
    "    \n",
    "    # 加权平均\n",
    "    prediction = (weights[0] * pred1 + \n",
    "                 weights[1] * pred2 + \n",
    "                 weights[2] * pred3)\n",
    "    \n",
    "    # 自适应范围限制\n",
    "    allowed_diff = 1.5 * (1 - reliability)\n",
    "    base = (u_stats['mean'] + i_stats['mean']) / 2\n",
    "    prediction = max(base - allowed_diff, min(base + allowed_diff, prediction))\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# 4. 预测\n",
    "print(\"开始预测...\")\n",
    "with open(\"predictions_Rating.csv\", 'w') as predictions_rating:\n",
    "    predictions_rating.write(\"userID,bookID,rating\\n\")\n",
    "    \n",
    "    test_pairs = []\n",
    "    with open(\"pairs_Rating.csv\") as f:\n",
    "        next(f)\n",
    "        for l in f:\n",
    "            test_pairs.append(l.strip().split(','))\n",
    "    \n",
    "    for u, b in tqdm(test_pairs, desc=\"Predicting ratings\"):\n",
    "        try:\n",
    "            prediction = ensemble_predict(u, b)\n",
    "            \n",
    "            # 最终范围限制\n",
    "            prediction = max(1, min(5, prediction))\n",
    "            \n",
    "        except:\n",
    "            # 使用最保守的预测\n",
    "            prediction = global_mean\n",
    "            prediction = max(1, min(5, prediction))\n",
    "        \n",
    "        predictions_rating.write(f\"{u},{b},{int(round(prediction))}\\n\")\n",
    "\n",
    "print(\"评分预测完成，结果已保存到 predictions_Rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算扩展统计信息...\n",
      "开始预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ratings: 100%|██████████| 10000/10000 [00:00<00:00, 68694.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评分预测完成，结果已保存到 predictions_Rating.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"计算扩展统计信息...\")\n",
    "rating_distribution = {}\n",
    "for r in ratings:\n",
    "    r = int(round(r))\n",
    "    rating_distribution[r] = rating_distribution.get(r, 0) + 1\n",
    "\n",
    "total_ratings = sum(rating_distribution.values())\n",
    "rating_probs = {r: count/total_ratings for r, count in rating_distribution.items()}\n",
    "\n",
    "# 计算每个用户的评分分布\n",
    "user_rating_dist = defaultdict(lambda: defaultdict(int))\n",
    "for u, r in zip(users, ratings):\n",
    "    r = int(round(float(r)))\n",
    "    user_rating_dist[u][r] += 1\n",
    "\n",
    "def baseline1(user, item):\n",
    "    \"\"\"基于加权平均的基线预测器\"\"\"\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        w_u = np.tanh(u_stats['count'] / 10)\n",
    "        w_i = np.tanh(i_stats['count'] / 10)\n",
    "        pred = global_mean + \\\n",
    "               w_u * (u_stats['mean'] - global_mean) + \\\n",
    "               w_i * (i_stats['mean'] - global_mean)\n",
    "        return pred\n",
    "    elif user in user_stats:\n",
    "        return 0.85 * user_stats[user]['mean'] + 0.15 * global_mean\n",
    "    elif item in item_stats:\n",
    "        return 0.85 * item_stats[item]['mean'] + 0.15 * global_mean\n",
    "    return global_mean\n",
    "\n",
    "def baseline2(user, item):\n",
    "    \"\"\"基于标准差的可靠性预测器\"\"\"\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        u_reliability = 1 / (1 + u_stats['std'])\n",
    "        i_reliability = 1 / (1 + i_stats['std'])\n",
    "        weights_sum = u_reliability + i_reliability\n",
    "        if weights_sum > 0:\n",
    "            pred = (u_reliability * u_stats['mean'] + \n",
    "                   i_reliability * i_stats['mean']) / weights_sum\n",
    "            return 0.85 * pred + 0.15 * global_mean\n",
    "    return global_mean\n",
    "\n",
    "def baseline3(user, item):\n",
    "    \"\"\"基于评分数量的指数衰减预测器\"\"\"\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        w_u = 1 - np.exp(-u_stats['count'] / 15)\n",
    "        w_i = 1 - np.exp(-i_stats['count'] / 15)\n",
    "        pred = ((w_u * u_stats['mean'] + w_i * i_stats['mean']) / \n",
    "                (w_u + w_i) if (w_u + w_i) > 0 else global_mean)\n",
    "        return pred\n",
    "    return global_mean\n",
    "\n",
    "def baseline4(user, item):\n",
    "    \"\"\"保守预测器\"\"\"\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        # 向全局均值收缩\n",
    "        shrinkage = 0.85\n",
    "        u_pred = shrinkage * u_stats['mean'] + (1 - shrinkage) * global_mean\n",
    "        i_pred = shrinkage * i_stats['mean'] + (1 - shrinkage) * global_mean\n",
    "        return (u_pred + i_pred) / 2\n",
    "    return global_mean\n",
    "\n",
    "def ensemble_predict(user, item):\n",
    "    # 获取所有基线预测\n",
    "    preds = [\n",
    "        baseline1(user, item),\n",
    "        baseline2(user, item),\n",
    "        baseline3(user, item),\n",
    "        baseline4(user, item)\n",
    "    ]\n",
    "    \n",
    "    # 获取统计信息\n",
    "    u_stats = user_stats.get(user, {'std': 1.0, 'mean': global_mean, 'count': 0})\n",
    "    i_stats = item_stats.get(item, {'std': 1.0, 'mean': global_mean, 'count': 0})\n",
    "    \n",
    "    # 计算可信度\n",
    "    rating_confidence = np.tanh((u_stats['count'] + i_stats['count']) / 20)\n",
    "    std_confidence = 1 / (1 + u_stats['std'] + i_stats['std'])\n",
    "    \n",
    "    # 综合可信度\n",
    "    reliability = (rating_confidence + std_confidence) / 2\n",
    "    \n",
    "    # 根据可信度分配权重\n",
    "    if reliability > 0.8:\n",
    "        weights = [0.35, 0.25, 0.2, 0.2]  # 高可信度时更信任baseline1\n",
    "    elif reliability > 0.5:\n",
    "        weights = [0.3, 0.3, 0.2, 0.2]    # 中等可信度时平均权重\n",
    "    else:\n",
    "        weights = [0.2, 0.3, 0.25, 0.25]  # 低可信度时更信任其他预测器\n",
    "    \n",
    "    # 加权平均\n",
    "    prediction = sum(w * p for w, p in zip(weights, preds))\n",
    "    \n",
    "    # 自适应范围限制\n",
    "    allowed_diff = 1.2 * (1 - reliability)\n",
    "    base = (u_stats['mean'] + i_stats['mean']) / 2\n",
    "    prediction = max(base - allowed_diff, min(base + allowed_diff, prediction))\n",
    "    \n",
    "    return max(1, min(5, prediction))\n",
    "\n",
    "# 4. 预测（保持不变）\n",
    "print(\"开始预测...\")\n",
    "with open(\"predictions_Rating.csv\", 'w') as predictions_rating:\n",
    "    predictions_rating.write(\"userID,bookID,prediction\\n\")\n",
    "    \n",
    "    test_pairs = []\n",
    "    with open(\"pairs_Rating.csv\") as f:\n",
    "        next(f)\n",
    "        for l in f:\n",
    "            test_pairs.append(l.strip().split(','))\n",
    "    \n",
    "    for u, b in tqdm(test_pairs, desc=\"Predicting ratings\"):\n",
    "        try:\n",
    "            prediction = ensemble_predict(u, b)\n",
    "        except:\n",
    "            prediction = global_mean\n",
    "        predictions_rating.write(f\"{u},{b},{int(round(prediction))}\\n\")\n",
    "\n",
    "print(\"评分预测完成，结果已保存到 predictions_Rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline1(user, item):\n",
    "    \"\"\"基于加权平均的基线预测器\"\"\"\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        w_u = np.tanh(u_stats['count'] / 9)  # 微调阈值\n",
    "        w_i = np.tanh(i_stats['count'] / 11)\n",
    "        pred = global_mean + \\\n",
    "               w_u * (u_stats['mean'] - global_mean) + \\\n",
    "               w_i * (i_stats['mean'] - global_mean)\n",
    "        return pred\n",
    "    elif user in user_stats:\n",
    "        return 0.87 * user_stats[user]['mean'] + 0.13 * global_mean\n",
    "    elif item in item_stats:\n",
    "        return 0.83 * item_stats[item]['mean'] + 0.17 * global_mean\n",
    "    return global_mean\n",
    "\n",
    "def baseline2(user, item):\n",
    "    \"\"\"基于标准差的可靠性预测器\"\"\"\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        u_reliability = 1 / (1 + 0.9 * u_stats['std'])\n",
    "        i_reliability = 1 / (1 + 0.9 * i_stats['std'])\n",
    "        weights_sum = u_reliability + i_reliability\n",
    "        if weights_sum > 0:\n",
    "            pred = (u_reliability * u_stats['mean'] + \n",
    "                   i_reliability * i_stats['mean']) / weights_sum\n",
    "            return 0.86 * pred + 0.14 * global_mean\n",
    "    return global_mean\n",
    "\n",
    "def baseline3(user, item):\n",
    "    \"\"\"基于评分数量的指数衰减预测器\"\"\"\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        w_u = 1 - np.exp(-u_stats['count'] / 14)\n",
    "        w_i = 1 - np.exp(-i_stats['count'] / 16)\n",
    "        pred = ((w_u * u_stats['mean'] + w_i * i_stats['mean']) / \n",
    "                (w_u + w_i) if (w_u + w_i) > 0 else global_mean)\n",
    "        return pred\n",
    "    return global_mean\n",
    "\n",
    "def baseline4(user, item):\n",
    "    \"\"\"保守预测器\"\"\"\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        shrinkage = 0.84  # 微调收缩率\n",
    "        u_pred = shrinkage * u_stats['mean'] + (1 - shrinkage) * global_mean\n",
    "        i_pred = shrinkage * i_stats['mean'] + (1 - shrinkage) * global_mean\n",
    "        return (u_pred + i_pred) / 2\n",
    "    return global_mean\n",
    "\n",
    "# ... (保持其他预测器不变)\n",
    "\n",
    "def ensemble_predict(user, item):\n",
    "    preds = [\n",
    "        baseline1(user, item),\n",
    "        baseline2(user, item),\n",
    "        baseline3(user, item),\n",
    "        baseline4(user, item)\n",
    "    ]\n",
    "    \n",
    "    u_stats = user_stats.get(user, {'std': 1.0, 'mean': global_mean, 'count': 0})\n",
    "    i_stats = item_stats.get(item, {'std': 1.0, 'mean': global_mean, 'count': 0})\n",
    "    \n",
    "    # 调整可信度计算\n",
    "    rating_confidence = np.tanh((u_stats['count'] + i_stats['count']) / 19)\n",
    "    std_confidence = 1 / (1 + 0.95 * (u_stats['std'] + i_stats['std']))\n",
    "    \n",
    "    # 添加均值差异的影响，但权重很小\n",
    "    mean_diff = abs(u_stats['mean'] - i_stats['mean']) / 5\n",
    "    mean_confidence = 1 - mean_diff\n",
    "    \n",
    "    # 综合可信度计算\n",
    "    reliability = (0.5 * rating_confidence + 0.4 * std_confidence + 0.1 * mean_confidence)\n",
    "    \n",
    "    # 更细致的权重分配\n",
    "    if reliability > 0.85:  # 极高可信度\n",
    "        weights = [0.4, 0.25, 0.175, 0.175]\n",
    "    elif reliability > 0.7:  # 高可信度\n",
    "        weights = [0.35, 0.25, 0.2, 0.2]\n",
    "    elif reliability > 0.5:  # 中等可信度\n",
    "        weights = [0.3, 0.3, 0.2, 0.2]\n",
    "    else:  # 低可信度\n",
    "        weights = [0.2, 0.3, 0.25, 0.25]\n",
    "    \n",
    "    prediction = sum(w * p for w, p in zip(weights, preds))\n",
    "    \n",
    "    # 根据可信度调整范围限制\n",
    "    if reliability > 0.85:\n",
    "        allowed_diff = 1.0 * (1 - reliability)  # 极高可信度时更严格的限制\n",
    "    else:\n",
    "        allowed_diff = 1.15 * (1 - reliability)\n",
    "    \n",
    "    base = (u_stats['mean'] + i_stats['mean']) / 2\n",
    "    prediction = max(base - allowed_diff, min(base + allowed_diff, prediction))\n",
    "    \n",
    "    return max(1, min(5, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ratings: 100%|██████████| 10000/10000 [00:00<00:00, 68965.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评分预测完成，结果已保存到 predictions_Rating.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. 预测（保持不变）\n",
    "print(\"开始预测...\")\n",
    "with open(\"predictions_Rating.csv\", 'w') as predictions_rating:\n",
    "    predictions_rating.write(\"userID,bookID,prediction\\n\")\n",
    "    \n",
    "    test_pairs = []\n",
    "    with open(\"pairs_Rating.csv\") as f:\n",
    "        next(f)\n",
    "        for l in f:\n",
    "            test_pairs.append(l.strip().split(','))\n",
    "    \n",
    "    for u, b in tqdm(test_pairs, desc=\"Predicting ratings\"):\n",
    "        try:\n",
    "            prediction = ensemble_predict(u, b)\n",
    "        except:\n",
    "            prediction = global_mean\n",
    "        predictions_rating.write(f\"{u},{b},{int(round(prediction))}\\n\")\n",
    "\n",
    "print(\"评分预测完成，结果已保存到 predictions_Rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始进行评分预测...\n",
      "预处理数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190000/190000 [00:00<00:00, 370754.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算统计特征...\n",
      "构建评分矩阵...\n",
      "执行矩阵分解...\n",
      "准备训练数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190000/190000 [00:02<00:00, 78556.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练模型...\n",
      "开始预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1016.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评分预测完成，结果已保存到 predictions_Rating.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"开始进行评分预测...\")\n",
    "\n",
    "# 1. 数据预处理\n",
    "print(\"预处理数据...\")\n",
    "user_ratings = defaultdict(list)\n",
    "item_ratings = defaultdict(list)\n",
    "rating_matrix_data = []\n",
    "\n",
    "# 收集评分数据\n",
    "for u, b, r in tqdm(allRatings):\n",
    "    try:\n",
    "        rating = float(r)\n",
    "        user_ratings[u].append(rating)\n",
    "        item_ratings[b].append(rating)\n",
    "        rating_matrix_data.append((u, b, rating))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# 2. 计算统计特征\n",
    "print(\"计算统计特征...\")\n",
    "global_mean = np.mean([r for _, _, r in rating_matrix_data])\n",
    "\n",
    "user_stats = {}\n",
    "item_stats = {}\n",
    "\n",
    "for u, ratings in user_ratings.items():\n",
    "    user_stats[u] = {\n",
    "        'mean': np.mean(ratings),\n",
    "        'std': np.std(ratings) if len(ratings) > 1 else 0,\n",
    "        'count': len(ratings),\n",
    "        'median': np.median(ratings)\n",
    "    }\n",
    "\n",
    "for b, ratings in item_ratings.items():\n",
    "    item_stats[b] = {\n",
    "        'mean': np.mean(ratings),\n",
    "        'std': np.std(ratings) if len(ratings) > 1 else 0,\n",
    "        'count': len(ratings),\n",
    "        'median': np.median(ratings)\n",
    "    }\n",
    "\n",
    "# 3. 构建评分矩阵\n",
    "print(\"构建评分矩阵...\")\n",
    "users = sorted(list(user_ratings.keys()))\n",
    "items = sorted(list(item_ratings.keys()))\n",
    "user_to_idx = {u: i for i, u in enumerate(users)}\n",
    "item_to_idx = {i: j for j, i in enumerate(items)}\n",
    "\n",
    "rows, cols, data = [], [], []\n",
    "for u, b, r in rating_matrix_data:\n",
    "    if u in user_to_idx and b in item_to_idx:\n",
    "        rows.append(user_to_idx[u])\n",
    "        cols.append(item_to_idx[b])\n",
    "        data.append(float(r))\n",
    "\n",
    "rating_matrix = csr_matrix((data, (rows, cols)), shape=(len(users), len(items)))\n",
    "\n",
    "# 4. 矩阵分解\n",
    "print(\"执行矩阵分解...\")\n",
    "n_components = 50\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "user_features = svd.fit_transform(rating_matrix)\n",
    "item_features = svd.components_.T\n",
    "\n",
    "# 5. 特征工程\n",
    "def create_features(user, item, rating=None):\n",
    "    features = {}\n",
    "    \n",
    "    # 用户特征\n",
    "    u_stats = user_stats.get(user, {'mean': global_mean, 'std': 0, 'count': 0, 'median': global_mean})\n",
    "    features['user_mean'] = u_stats['mean']\n",
    "    features['user_std'] = u_stats['std']\n",
    "    features['user_count'] = np.log1p(u_stats['count'])\n",
    "    features['user_median'] = u_stats['median']\n",
    "    \n",
    "    # 物品特征\n",
    "    i_stats = item_stats.get(item, {'mean': global_mean, 'std': 0, 'count': 0, 'median': global_mean})\n",
    "    features['item_mean'] = i_stats['mean']\n",
    "    features['item_std'] = i_stats['std']\n",
    "    features['item_count'] = np.log1p(i_stats['count'])\n",
    "    features['item_median'] = i_stats['median']\n",
    "    \n",
    "    # 交互特征\n",
    "    features['mean_diff'] = abs(u_stats['mean'] - i_stats['mean'])\n",
    "    features['std_sum'] = u_stats['std'] + i_stats['std']\n",
    "    features['count_interaction'] = np.log1p(u_stats['count'] * i_stats['count'])\n",
    "    \n",
    "    # 矩阵分解特征\n",
    "    try:\n",
    "        u_idx = user_to_idx.get(user)\n",
    "        i_idx = item_to_idx.get(item)\n",
    "        if u_idx is not None and i_idx is not None:\n",
    "            mf_pred = np.dot(user_features[u_idx], item_features[i_idx])\n",
    "            features['mf_pred'] = mf_pred\n",
    "            \n",
    "            # 添加部分潜在因子作为特征\n",
    "            for i in range(5):  # 使用前5个潜在因子\n",
    "                features[f'user_factor_{i}'] = user_features[u_idx][i]\n",
    "                features[f'item_factor_{i}'] = item_features[i_idx][i]\n",
    "    except:\n",
    "        features['mf_pred'] = global_mean\n",
    "        for i in range(5):\n",
    "            features[f'user_factor_{i}'] = 0\n",
    "            features[f'item_factor_{i}'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 6. 准备训练数据\n",
    "print(\"准备训练数据...\")\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "for u, b, r in tqdm(rating_matrix_data):\n",
    "    features = create_features(u, b, r)\n",
    "    train_features.append(features)\n",
    "    train_labels.append(float(r))\n",
    "\n",
    "# 转换为DataFrame\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# 7. 训练模型\n",
    "print(\"训练模型...\")\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "model.fit(train_df, train_labels)\n",
    "\n",
    "# 8. 预测\n",
    "print(\"开始预测...\")\n",
    "with open(\"predictions_Rating.csv\", 'w') as predictions_rating:\n",
    "    predictions_rating.write(\"userID,bookID,prediction\\n\")\n",
    "    \n",
    "    test_pairs = []\n",
    "    with open(\"pairs_Rating.csv\") as f:\n",
    "        next(f)\n",
    "        for l in f:\n",
    "            test_pairs.append(l.strip().split(','))\n",
    "    \n",
    "    for u, b in tqdm(test_pairs):\n",
    "        try:\n",
    "            features = create_features(u, b)\n",
    "            features_df = pd.DataFrame([features])\n",
    "            prediction = model.predict(features_df)[0]\n",
    "            \n",
    "            # 智能范围限制\n",
    "            u_stats = user_stats.get(u, {'mean': global_mean, 'std': 1.0})\n",
    "            i_stats = item_stats.get(b, {'mean': global_mean, 'std': 1.0})\n",
    "            \n",
    "            base = (u_stats['mean'] + i_stats['mean']) / 2\n",
    "            allowed_diff = 1.0 + 0.5 * (u_stats['std'] + i_stats['std']) / 2\n",
    "            \n",
    "            prediction = max(base - allowed_diff, min(base + allowed_diff, prediction))\n",
    "            prediction = max(1, min(5, prediction))\n",
    "            \n",
    "        except:\n",
    "            prediction = global_mean\n",
    "            prediction = max(1, min(5, prediction))\n",
    "        \n",
    "        predictions_rating.write(f\"{u},{b},{int(round(prediction))}\\n\")\n",
    "\n",
    "print(\"评分预测完成，结果已保存到 predictions_Rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始进行评分预测...\n",
      "预处理数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:00<00:00, 891702.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算统计特征...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建评分矩阵...\n",
      "执行矩阵分解...\n",
      "准备训练数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:02<00:00, 84533.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练模型...\n",
      "开始预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1038.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评分预测完成，结果已保存到 predictions_Rating.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"开始进行评分预测...\")\n",
    "\n",
    "# 1. 数据预处理（保持不变）\n",
    "print(\"预处理数据...\")\n",
    "user_ratings = defaultdict(list)\n",
    "item_ratings = defaultdict(list)\n",
    "rating_matrix_data = []\n",
    "\n",
    "for u, b, r in tqdm(allRatings):\n",
    "    try:\n",
    "        rating = float(r)\n",
    "        user_ratings[u].append(rating)\n",
    "        item_ratings[b].append(rating)\n",
    "        rating_matrix_data.append((u, b, rating))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# 2. 计算统计特征\n",
    "print(\"计算统计特征...\")\n",
    "global_mean = np.mean([r for _, _, r in rating_matrix_data])\n",
    "\n",
    "user_stats = {}\n",
    "item_stats = {}\n",
    "\n",
    "for u, ratings in user_ratings.items():\n",
    "    ratings_array = np.array(ratings)\n",
    "    user_stats[u] = {\n",
    "        'mean': np.mean(ratings),\n",
    "        'std': np.std(ratings) if len(ratings) > 1 else 0,\n",
    "        'count': len(ratings),\n",
    "        'median': np.median(ratings),\n",
    "        'min': np.min(ratings),\n",
    "        'max': np.max(ratings),\n",
    "        'q25': np.percentile(ratings, 25),\n",
    "        'q75': np.percentile(ratings, 75)\n",
    "    }\n",
    "\n",
    "for b, ratings in item_ratings.items():\n",
    "    ratings_array = np.array(ratings)\n",
    "    item_stats[b] = {\n",
    "        'mean': np.mean(ratings),\n",
    "        'std': np.std(ratings) if len(ratings) > 1 else 0,\n",
    "        'count': len(ratings),\n",
    "        'median': np.median(ratings),\n",
    "        'min': np.min(ratings),\n",
    "        'max': np.max(ratings),\n",
    "        'q25': np.percentile(ratings, 25),\n",
    "        'q75': np.percentile(ratings, 75)\n",
    "    }\n",
    "\n",
    "# 3. 构建评分矩阵\n",
    "print(\"构建评分矩阵...\")\n",
    "users = sorted(list(user_ratings.keys()))\n",
    "items = sorted(list(item_ratings.keys()))\n",
    "user_to_idx = {u: i for i, u in enumerate(users)}\n",
    "item_to_idx = {i: j for j, i in enumerate(items)}\n",
    "\n",
    "rows, cols, data = [], [], []\n",
    "for u, b, r in rating_matrix_data:\n",
    "    if u in user_to_idx and b in item_to_idx:\n",
    "        rows.append(user_to_idx[u])\n",
    "        cols.append(item_to_idx[b])\n",
    "        data.append(float(r) - global_mean)  # 中心化\n",
    "\n",
    "rating_matrix = csr_matrix((data, (rows, cols)), shape=(len(users), len(items)))\n",
    "\n",
    "# 4. 矩阵分解\n",
    "print(\"执行矩阵分解...\")\n",
    "n_components = 30  # 减少组件数量\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "user_features = svd.fit_transform(rating_matrix)\n",
    "item_features = svd.components_.T\n",
    "\n",
    "# 5. 基线预测器\n",
    "def baseline_predictor(user, item):\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        \n",
    "        # 计算可靠性权重\n",
    "        u_weight = np.tanh(u_stats['count'] / 10)\n",
    "        i_weight = np.tanh(i_stats['count'] / 10)\n",
    "        \n",
    "        # 考虑评分范围\n",
    "        u_range = u_stats['max'] - u_stats['min']\n",
    "        i_range = i_stats['max'] - i_stats['min']\n",
    "        range_weight = 1 / (1 + 0.1 * (u_range + i_range))\n",
    "        \n",
    "        pred = global_mean + \\\n",
    "               u_weight * (u_stats['mean'] - global_mean) + \\\n",
    "               i_weight * (i_stats['mean'] - global_mean)\n",
    "        \n",
    "        return pred * range_weight + global_mean * (1 - range_weight)\n",
    "    elif user in user_stats:\n",
    "        return 0.8 * user_stats[user]['mean'] + 0.2 * global_mean\n",
    "    elif item in item_stats:\n",
    "        return 0.8 * item_stats[item]['mean'] + 0.2 * global_mean\n",
    "    return global_mean\n",
    "\n",
    "# 6. 特征工程\n",
    "def create_features(user, item, rating=None):\n",
    "    features = {}\n",
    "    \n",
    "    # 基线预测\n",
    "    features['baseline_pred'] = baseline_predictor(user, item)\n",
    "    \n",
    "    # 用户特征\n",
    "    u_stats = user_stats.get(user, {\n",
    "        'mean': global_mean, 'std': 0, 'count': 0, 'median': global_mean,\n",
    "        'min': global_mean, 'max': global_mean, 'q25': global_mean, 'q75': global_mean\n",
    "    })\n",
    "    \n",
    "    features['user_mean'] = u_stats['mean']\n",
    "    features['user_std'] = u_stats['std']\n",
    "    features['user_count'] = np.log1p(u_stats['count'])\n",
    "    features['user_range'] = u_stats['max'] - u_stats['min']\n",
    "    features['user_iqr'] = u_stats['q75'] - u_stats['q25']\n",
    "    \n",
    "    # 物品特征\n",
    "    i_stats = item_stats.get(item, {\n",
    "        'mean': global_mean, 'std': 0, 'count': 0, 'median': global_mean,\n",
    "        'min': global_mean, 'max': global_mean, 'q25': global_mean, 'q75': global_mean\n",
    "    })\n",
    "    \n",
    "    features['item_mean'] = i_stats['mean']\n",
    "    features['item_std'] = i_stats['std']\n",
    "    features['item_count'] = np.log1p(i_stats['count'])\n",
    "    features['item_range'] = i_stats['max'] - i_stats['min']\n",
    "    features['item_iqr'] = i_stats['q75'] - i_stats['q25']\n",
    "    \n",
    "    # 矩阵分解特征\n",
    "    try:\n",
    "        u_idx = user_to_idx.get(user)\n",
    "        i_idx = item_to_idx.get(item)\n",
    "        if u_idx is not None and i_idx is not None:\n",
    "            mf_pred = np.dot(user_features[u_idx], item_features[i_idx]) + global_mean\n",
    "            features['mf_pred'] = mf_pred\n",
    "    except:\n",
    "        features['mf_pred'] = global_mean\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 7. 训练模型\n",
    "print(\"准备训练数据...\")\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "for u, b, r in tqdm(rating_matrix_data):\n",
    "    features = create_features(u, b, r)\n",
    "    train_features.append(features)\n",
    "    train_labels.append(float(r))\n",
    "\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "print(\"训练模型...\")\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mse',\n",
    "    'num_leaves': 16,  # 减少叶子节点\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 20,  # 增加最小样本数\n",
    "    'reg_alpha': 0.1,  # L1正则化\n",
    "    'reg_lambda': 0.1,  # L2正则化\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model = lgb.LGBMRegressor(**params, n_estimators=100)\n",
    "model.fit(train_df, train_labels)\n",
    "\n",
    "# 8. 预测\n",
    "print(\"开始预测...\")\n",
    "with open(\"predictions_Rating.csv\", 'w') as predictions_rating:\n",
    "    predictions_rating.write(\"userID,bookID,rating\\n\")\n",
    "    \n",
    "    test_pairs = []\n",
    "    with open(\"pairs_Rating.csv\") as f:\n",
    "        next(f)\n",
    "        for l in f:\n",
    "            test_pairs.append(l.strip().split(','))\n",
    "    \n",
    "    for u, b in tqdm(test_pairs):\n",
    "        try:\n",
    "            features = create_features(u, b)\n",
    "            features_df = pd.DataFrame([features])\n",
    "            \n",
    "            # 获取基线和模型预测\n",
    "            baseline = features['baseline_pred']\n",
    "            model_pred = model.predict(features_df)[0]\n",
    "            \n",
    "            # 加权组合预测\n",
    "            w_model = 0.3  # 降低模型权重\n",
    "            prediction = w_model * model_pred + (1 - w_model) * baseline\n",
    "            \n",
    "            # 范围限制\n",
    "            u_stats = user_stats.get(u, {'mean': global_mean, 'std': 1.0})\n",
    "            i_stats = item_stats.get(b, {'mean': global_mean, 'std': 1.0})\n",
    "            \n",
    "            base = (u_stats['mean'] + i_stats['mean']) / 2\n",
    "            allowed_diff = 0.8  # 收紧范围限制\n",
    "            \n",
    "            prediction = max(base - allowed_diff, min(base + allowed_diff, prediction))\n",
    "            prediction = max(1, min(5, prediction))\n",
    "            \n",
    "        except:\n",
    "            prediction = global_mean\n",
    "            prediction = max(1, min(5, prediction))\n",
    "        \n",
    "        predictions_rating.write(f\"{u},{b},{int(round(prediction))}\\n\")\n",
    "\n",
    "print(\"评分预测完成，结果已保存到 predictions_Rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始进行评分预测...\n",
      "预处理数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/190000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190000/190000 [00:00<00:00, 490925.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练模型...\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "开始预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 17905.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评分预测完成，结果已保存到 predictions_Rating.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from surprise import Dataset, Reader, SVDpp, SVD, KNNWithMeans, SlopeOne\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "print(\"开始进行评分预测...\")\n",
    "\n",
    "# 1. 数据预处理\n",
    "print(\"预处理数据...\")\n",
    "training_data = []\n",
    "for u, b, r in tqdm(ratingsTrain):\n",
    "    try:\n",
    "        training_data.append([u, b, float(r)])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# 2. 计算统计信息\n",
    "user_ratings = defaultdict(list)\n",
    "item_ratings = defaultdict(list)\n",
    "global_mean = np.mean([r for _, _, r in training_data])\n",
    "\n",
    "for u, b, r in training_data:\n",
    "    user_ratings[u].append(r)\n",
    "    item_ratings[b].append(r)\n",
    "\n",
    "user_stats = {}\n",
    "for u, ratings in user_ratings.items():\n",
    "    ratings = np.array(ratings)\n",
    "    user_stats[u] = {\n",
    "        'mean': np.mean(ratings),\n",
    "        'std': np.std(ratings) if len(ratings) > 1 else 0,\n",
    "        'count': len(ratings),\n",
    "        'median': np.median(ratings),\n",
    "        'q25': np.percentile(ratings, 25),\n",
    "        'q75': np.percentile(ratings, 75)\n",
    "    }\n",
    "\n",
    "item_stats = {}\n",
    "for b, ratings in item_ratings.items():\n",
    "    ratings = np.array(ratings)\n",
    "    item_stats[b] = {\n",
    "        'mean': np.mean(ratings),\n",
    "        'std': np.std(ratings) if len(ratings) > 1 else 0,\n",
    "        'count': len(ratings),\n",
    "        'median': np.median(ratings),\n",
    "        'q25': np.percentile(ratings, 25),\n",
    "        'q75': np.percentile(ratings, 75)\n",
    "    }\n",
    "\n",
    "# 3. 基线预测器\n",
    "def baseline1(user, item):\n",
    "    \"\"\"基于加权平均的预测器\"\"\"\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        w_u = np.tanh(u_stats['count'] / 10)\n",
    "        w_i = np.tanh(i_stats['count'] / 10)\n",
    "        pred = global_mean + \\\n",
    "               w_u * (u_stats['mean'] - global_mean) + \\\n",
    "               w_i * (i_stats['mean'] - global_mean)\n",
    "        return pred\n",
    "    elif user in user_stats:\n",
    "        return 0.85 * user_stats[user]['mean'] + 0.15 * global_mean\n",
    "    elif item in item_stats:\n",
    "        return 0.85 * item_stats[item]['mean'] + 0.15 * global_mean\n",
    "    return global_mean\n",
    "\n",
    "def baseline2(user, item):\n",
    "    \"\"\"基于四分位数的预测器\"\"\"\n",
    "    if user in user_stats and item in item_stats:\n",
    "        u_stats = user_stats[user]\n",
    "        i_stats = item_stats[item]\n",
    "        u_iqr = u_stats['q75'] - u_stats['q25']\n",
    "        i_iqr = i_stats['q75'] - i_stats['q25']\n",
    "        reliability = 1 / (1 + 0.5 * (u_iqr + i_iqr))\n",
    "        pred = reliability * (u_stats['median'] + i_stats['median']) / 2 + \\\n",
    "               (1 - reliability) * global_mean\n",
    "        return pred\n",
    "    return global_mean\n",
    "\n",
    "# 4. 创建Surprise数据格式\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(pd.DataFrame(training_data, columns=['user', 'item', 'rating']), reader)\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# 5. 训练多个模型\n",
    "print(\"训练模型...\")\n",
    "# SVD++\n",
    "svdpp = SVDpp(n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02)\n",
    "svdpp.fit(trainset)\n",
    "\n",
    "# 普通SVD\n",
    "svd = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02)\n",
    "svd.fit(trainset)\n",
    "\n",
    "# KNN\n",
    "knn = KNNWithMeans(k=40, min_k=1, sim_options={'name': 'pearson_baseline', 'user_based': False})\n",
    "knn.fit(trainset)\n",
    "\n",
    "# SlopeOne\n",
    "slopeone = SlopeOne()\n",
    "slopeone.fit(trainset)\n",
    "\n",
    "# 6. 预测函数\n",
    "def get_prediction(user, item):\n",
    "    try:\n",
    "        # 获取各个模型的预测\n",
    "        pred_svdpp = svdpp.predict(user, item).est\n",
    "        pred_svd = svd.predict(user, item).est\n",
    "        pred_knn = knn.predict(user, item).est\n",
    "        pred_slopeone = slopeone.predict(user, item).est\n",
    "        pred_base1 = baseline1(user, item)\n",
    "        pred_base2 = baseline2(user, item)\n",
    "        \n",
    "        # 获取统计信息\n",
    "        u_stats = user_stats.get(user, {'mean': global_mean, 'std': 1.0, 'count': 0})\n",
    "        i_stats = item_stats.get(item, {'mean': global_mean, 'std': 1.0, 'count': 0})\n",
    "        \n",
    "        # 计算可信度\n",
    "        rating_conf = np.tanh((u_stats['count'] + i_stats['count']) / 20)\n",
    "        std_conf = 1 / (1 + u_stats['std'] + i_stats['std'])\n",
    "        reliability = (rating_conf + std_conf) / 2\n",
    "        \n",
    "        # 根据可信度分配权重\n",
    "        if reliability > 0.8:\n",
    "            weights = [0.3, 0.2, 0.1, 0.1, 0.2, 0.1]  # 高可信度时更信任SVD++\n",
    "        elif reliability > 0.5:\n",
    "            weights = [0.25, 0.15, 0.15, 0.15, 0.15, 0.15]  # 中等可信度时平均分配\n",
    "        else:\n",
    "            weights = [0.2, 0.1, 0.1, 0.1, 0.25, 0.25]  # 低可信度时更信任基线预测器\n",
    "        \n",
    "        # 加权平均\n",
    "        predictions = [pred_svdpp, pred_svd, pred_knn, pred_slopeone, pred_base1, pred_base2]\n",
    "        pred = sum(w * p for w, p in zip(weights, predictions))\n",
    "        \n",
    "        # 智能范围限制\n",
    "        base = (u_stats['mean'] + i_stats['mean']) / 2\n",
    "        allowed_diff = 0.8 + 0.2 * (1 / (1 + u_stats['std'] + i_stats['std']))\n",
    "        pred = max(base - allowed_diff, min(base + allowed_diff, pred))\n",
    "        \n",
    "        return max(1, min(5, pred))\n",
    "    \n",
    "    except:\n",
    "        return global_mean\n",
    "\n",
    "# 7. 生成预测\n",
    "print(\"开始预测...\")\n",
    "with open(\"predictions_Rating.csv\", 'w') as predictions_rating:\n",
    "    predictions_rating.write(\"userID,bookID,prediction\\n\")\n",
    "    \n",
    "    test_pairs = []\n",
    "    with open(\"pairs_Rating.csv\") as f:\n",
    "        next(f)\n",
    "        for l in f:\n",
    "            test_pairs.append(l.strip().split(','))\n",
    "    \n",
    "    for u, b in tqdm(test_pairs):\n",
    "        try:\n",
    "            prediction = get_prediction(u, b)\n",
    "        except:\n",
    "            prediction = global_mean\n",
    "        \n",
    "        predictions_rating.write(f\"{u},{b},{int(round(prediction))}\\n\")\n",
    "\n",
    "print(\"评分预测完成，结果已保存到 predictions_Rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVDpp\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "print(\"开始进行评分预测...\")\n",
    "\n",
    "# 1. 数据预处理\n",
    "print(\"预处理数据...\")\n",
    "training_data = []\n",
    "for u, b, r in tqdm(ratingsTrain):\n",
    "    try:\n",
    "        training_data.append([u, b, float(r)])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# 2. 创建Surprise数据格式\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(pd.DataFrame(training_data, columns=['user', 'item', 'rating']), reader)\n",
    "\n",
    "# 3. 网格搜索最佳参数\n",
    "print(\"搜索最佳参数...\")\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100, 150],  # 潜在因子数量\n",
    "    'n_epochs': [30, 40],         # 训练轮数\n",
    "    'lr_all': [0.005, 0.007],     # 学习率\n",
    "    'reg_all': [0.02, 0.1],       # 正则化参数\n",
    "    'init_mean': [0],             # 初始化均值\n",
    "    'init_std_dev': [0.1]         # 初始化标准差\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(SVDpp, param_grid, measures=['rmse'], \n",
    "                 cv=3, n_jobs=-1, joblib_verbose=0)\n",
    "gs.fit(data)\n",
    "\n",
    "print(f\"最佳参数: {gs.best_params['rmse']}\")\n",
    "print(f\"最佳RMSE: {gs.best_score['rmse']}\")\n",
    "\n",
    "# 4. 使用最佳参数训练最终模型\n",
    "print(\"训练最终模型...\")\n",
    "best_params = gs.best_params['rmse']\n",
    "model = SVDpp(\n",
    "    n_factors=best_params['n_factors'],\n",
    "    n_epochs=best_params['n_epochs'],\n",
    "    lr_all=best_params['lr_all'],\n",
    "    reg_all=best_params['reg_all'],\n",
    "    init_mean=best_params['init_mean'],\n",
    "    init_std_dev=best_params['init_std_dev'],\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 在完整训练集上训练\n",
    "trainset = data.build_full_trainset()\n",
    "model.fit(trainset)\n",
    "\n",
    "# 5. 预测函数\n",
    "def get_prediction(user, item):\n",
    "    try:\n",
    "        prediction = model.predict(user, item).est\n",
    "        return max(1, min(5, prediction))  # 确保预测在1-5范围内\n",
    "    except:\n",
    "        return 3.0  # 如果预测失败，返回中间值\n",
    "\n",
    "# 6. 生成预测\n",
    "print(\"开始预测...\")\n",
    "with open(\"predictions_Rating.csv\", 'w') as predictions_rating:\n",
    "    predictions_rating.write(\"userID,bookID,rating\\n\")\n",
    "    \n",
    "    test_pairs = []\n",
    "    with open(\"pairs_Rating.csv\") as f:\n",
    "        next(f)\n",
    "        for l in f:\n",
    "            test_pairs.append(l.strip().split(','))\n",
    "    \n",
    "    for u, b in tqdm(test_pairs):\n",
    "        prediction = get_prediction(u, b)\n",
    "        predictions_rating.write(f\"{u},{b},{int(round(prediction))}\\n\")\n",
    "\n",
    "print(\"评分预测完成，结果已保存到 predictions_Rating.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
