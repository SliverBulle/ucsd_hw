{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import random\n",
    "import gzip\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x): # Checks that an answer is a float\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open(\"young_adult_10000.json.gz\")\n",
    "dataset = []\n",
    "for l in f:\n",
    "    dataset.append(json.loads(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '8842281e1d1347389f2ab93d60773d4d',\n",
       " 'book_id': '2767052',\n",
       " 'review_id': '248c011811e945eca861b5c31a549291',\n",
       " 'rating': 5,\n",
       " 'review_text': \"I cracked and finally picked this up. Very enjoyable quick read - couldn't put it down - it was like crack. \\n I'm a bit bothered by the lack of backstory of how Panem and the Hunger Games come about. It is just kind of explained away in a few paragraphs and we are left to accept this very strange world where teenagers are pitted into an arena each year to kill each other? I was expecting it because I've seen Battle Royale, but I would have appreciated knowing more of the backstory of how the world could have come into such a odd state. \\n I suppose what makes a book like this interesting is thinking about the strategy of it all. The players are going to be statistically encouraged to band together because they will last longer that way, but by definition of course any partnership will be broken, and the drama of how that unfolds is always interesting and full of friendships broken and betrayal. Each character approached the game in their own way. Some banded together in larger coalitions, some were loners initially and banded together later. And some were just loners, like Foxface. A lot depended on your survival skill: could you find food and water on your own? Self-dependence is highly valued - and of course our hero was strong there. \\n All in all, a fun read, but I feel kind of dirty for having read it.\",\n",
       " 'date_added': 'Wed Jan 13 13:38:25 -0800 2010',\n",
       " 'date_updated': 'Wed Mar 22 11:46:36 -0700 2017',\n",
       " 'read_at': 'Sun Mar 25 00:00:00 -0700 2012',\n",
       " 'started_at': 'Fri Mar 23 00:00:00 -0700 2012',\n",
       " 'n_votes': 24,\n",
       " 'n_comments': 25}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6885330408320325 [0.07109019]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def feature(datum):\n",
    "    X = np.array([[review['review_text'].count('!')] for review in datum])\n",
    "    y = np.array([review['rating'] for review in datum])\n",
    "\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = feature(dataset)\n",
    "theta1 = model.coef_\n",
    "theta0 = model.intercept_\n",
    "print(theta0, theta1)\n",
    "X = np.array([[review['review_text'].count('!')] for review in dataset])\n",
    "y = np.array([review['rating'] for review in dataset])\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "answers['Q1'] = [theta0, theta1[0], mse]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q1'], 3) # Check the format of your answer (three floats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7175128077972013 [-4.12150653e-05  7.52759173e-02]\n"
     ]
    }
   ],
   "source": [
    "def feature(datum):\n",
    "\n",
    "    X = np.array([[len(review['review_text']), review['review_text'].count('!')] for review in datum]) \n",
    "    y = np.array([review['rating'] for review in datum])\n",
    "\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "model = feature(dataset)\n",
    "theta1 = model.coef_\n",
    "theta0 = model.intercept_\n",
    "print(theta0, theta1)\n",
    "\n",
    "X = np.array([[len(review['review_text']), review['review_text'].count('!')] for review in dataset]) \n",
    "y = np.array([review['rating'] for review in dataset])\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "answers['Q2'] = [theta0, theta1[0],theta1[1], mse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q2'], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "def feature(datum, deg):\n",
    "\n",
    "    #mses \n",
    "    mses = []\n",
    "    # feature for a specific polynomial degree\n",
    "    X = np.array([[review['review_text'].count('!')] for review in datum])\n",
    "    y = np.array([review['rating'] for review in datum])\n",
    "    for i in range(1, deg + 1):\n",
    "        poly = PolynomialFeatures(degree = i)  \n",
    "        X_poly = poly.fit_transform(X)        \n",
    "\n",
    "        #train model\n",
    "        model = linear_model.LinearRegression()\n",
    "        model.fit(X_poly, y)\n",
    "        #predict\n",
    "        y_pred = model.predict(X_poly)\n",
    "\n",
    "        # mse\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        mses.append(mse)\n",
    "    return mses\n",
    "\n",
    "mses = feature(dataset, 5)\n",
    "\n",
    "answers['Q3'] = mses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q3'], 5)# List of length 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[review['review_text'].count('!')] for review in dataset])\n",
    "y = np.array([review['rating'] for review in dataset])\n",
    "\n",
    "#split data into training and testing\n",
    "X_train = X[:5000]\n",
    "X_test = X[5000:]\n",
    "y_train = y[:5000]\n",
    "y_test = y[5000:]\n",
    "\n",
    "def feature(X,y,X_test,y_test, deg):\n",
    "    mses = []\n",
    "    for i in range(1, deg + 1):\n",
    "        poly = PolynomialFeatures(degree = i)  \n",
    "        X_poly = poly.fit_transform(X_train)        \n",
    "\n",
    "        #train model\n",
    "        model = linear_model.LinearRegression()\n",
    "        model.fit(X_poly, y_train)\n",
    "        #predict\n",
    "        X_poly_test = poly.fit_transform(X_test)\n",
    "        y_pred = model.predict(X_poly_test)\n",
    "\n",
    "        # mse\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mses.append(mse)\n",
    "    return mses\n",
    "\n",
    "mses = feature(X_train, y_train, X_test, y_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q4'] = mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q4'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 计算MAE\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "\n",
    "answers['Q5'] = mae\n",
    "assertFloat(answers['Q5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"beer_50000.json\")\n",
    "dataset = []\n",
    "for l in f:\n",
    "    if 'user/gender' in l:\n",
    "        dataset.append(eval(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20403"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review/appearance 4.0\n",
      "beer/style American Double / Imperial IPA\n",
      "review/palate 4.0\n",
      "review/taste 4.5\n",
      "beer/name Cauldron DIPA\n",
      "review/timeUnix 1293735206\n",
      "user/gender Male\n",
      "user/birthdayRaw Jun 16, 1901\n",
      "beer/ABV 7.7\n",
      "beer/beerId 64883\n",
      "user/birthdayUnix -2163081600\n",
      "beer/brewerId 1075\n",
      "review/timeStruct {'isdst': 0, 'mday': 30, 'hour': 18, 'min': 53, 'sec': 26, 'mon': 12, 'year': 2010, 'yday': 364, 'wday': 3}\n",
      "user/ageInSeconds 3581417047\n",
      "review/overall 4.0\n",
      "review/text According to the website, the style for the Caldera Cauldron changes every year. The current release is a DIPA, which frankly is the only cauldron I'm familiar with (it was an IPA/DIPA the last time I ordered a cauldron at the horsebrass several years back). In any event... at the Horse Brass yesterday.\t\tThe beer pours an orange copper color with good head retention and lacing. The nose is all hoppy IPA goodness, showcasing a huge aroma of dry citrus, pine and sandlewood. The flavor profile replicates the nose pretty closely in this West Coast all the way DIPA. This DIPA is not for the faint of heart and is a bit much even for a hophead like myslf. The finish is quite dry and hoppy, and there's barely enough sweet malt to balance and hold up the avalanche of hoppy bitterness in this beer. Mouthfeel is actually fairly light, with a long, persistentely bitter finish. Drinkability is good, with the alcohol barely noticeable in this well crafted beer. Still, this beer is so hugely hoppy/bitter, it's really hard for me to imagine ordering more than a single glass. Regardless, this is a very impressive beer from the folks at Caldera.\n",
      "user/profileName johnmichaelsen\n",
      "review/aroma 4.5\n"
     ]
    }
   ],
   "source": [
    "for k,v in dataset[0].items():\n",
    "    print(k,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[review['review/text'].count('!')] for review in dataset])\n",
    "y = np.array([1 if review['user/gender'] == 'Female' else 0 for review in dataset])\n",
    "\n",
    "\n",
    "\n",
    "def feature(X,y):\n",
    "    model = linear_model.LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "model = feature(X,y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "fpr = fp / (fp + tn)  # False Positive Rate\n",
    "fnr = fn / (fn + tp)  # False Negative Rate\n",
    "ber = (fpr + fnr) / 2\n",
    "\n",
    "answers['Q6'] = [tp,tn,fp,fn,ber]\n",
    "assertFloatList(answers['Q6'], 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrain the regressor using the class weight=’balanced’ option, \n",
    "# and report the same error metrics as above.\n",
    "\n",
    "def feature(X,y):\n",
    "    model = linear_model.LogisticRegression(class_weight='balanced')\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "model = feature(X,y)\n",
    "y_pred = model.predict(X)\n",
    "#print(y_pred[0:10])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "fpr = fp / (fp + tn)  # False Positive Rate\n",
    "fnr = fn / (fn + tp)  # False Negative Rate\n",
    "ber = (fpr + fnr) / 2\n",
    "\n",
    "answers['Q7'] = [tp,tn,fp,fn,ber]\n",
    "assertFloatList(answers['Q7'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.02, 0.025, 0.017]\n"
     ]
    }
   ],
   "source": [
    "#Report the precision@K of your balanced classifier for K ∈ [1, 10, 100, 1000, 10000] (your answer should\n",
    "#be a list of five precision values).\n",
    "y_probs = model.predict_proba(X)[:, 1]  # p(y=1|x)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# precision @ K\n",
    "precision_at_k = []\n",
    "ks = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "for k in ks:\n",
    "    # top k predictions\n",
    "    indices = np.argsort(y_probs)[-k:]  \n",
    "    true_positives = np.sum(y[indices] == 1)  \n",
    "    precision = true_positives / k  # precision@k\n",
    "    precision_at_k.append(precision)\n",
    "\n",
    "# precision@k\n",
    "print(precision_at_k)\n",
    "\n",
    "answers['Q8'] = precision_at_k\n",
    "assertFloatList(answers['Q8'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw1.txt\", 'w') # Write your answers to a file\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 [3.6885330408320325, 0.07109019019954116, 1.5231747404538287]\n",
      "Q2 [3.7175128077972013, -4.1215065294879717e-05, 0.07527591733232616, 1.5214029246165832]\n",
      "Q3 [1.5231747404538287, 1.5046686106250915, 1.4966845515179232, 1.490447730223069, 1.4896106953961648]\n",
      "Q4 [1.5248743859866298, 1.4977199259322431, 1.4856632190311343, 1.4767337440077455, 1.4809577272113095]\n",
      "Q5 0.9612280163687501\n",
      "Q6 [0, 20095, 0, 308, 0.5]\n",
      "Q7 [88, 16332, 3763, 220, 0.4507731134255145]\n",
      "Q8 [0.0, 0.0, 0.02, 0.025, 0.017]\n"
     ]
    }
   ],
   "source": [
    "for k,v in answers.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
